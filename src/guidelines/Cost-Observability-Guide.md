# Cost Observability ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ë¹„ìš© ê³„ì‚° ë¡œì§](#ë¹„ìš©-ê³„ì‚°-ë¡œì§)
3. [API ëª…ì„¸](#api-ëª…ì„¸)
4. [ì˜ˆì‚° ê´€ë¦¬ ì „ëµ](#ì˜ˆì‚°-ê´€ë¦¬-ì „ëµ)
5. [ë¹„ìš© ìµœì í™” ë°©ë²•](#ë¹„ìš©-ìµœì í™”-ë°©ë²•)
6. [êµ¬í˜„ ê°€ì´ë“œ](#êµ¬í˜„-ê°€ì´ë“œ)

---

## ê°œìš”

### ë¬¸ì œ ì •ì˜

REXëŠ” **LLM Judge ê¸°ë°˜ í‰ê°€ ì§€í‘œ**ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë§‰ëŒ€í•œ LLM API ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤:

```
ë‹¨ì¼ í‰ê°€ ë¹„ìš© = QA ê°œìˆ˜ Ã— ì§€í‘œ ê°œìˆ˜ Ã— ì§€í‘œë‹¹ í‰ê·  ë¹„ìš©

ì˜ˆì‹œ:
- 150ê°œ QA Ã— 12ê°œ ì§€í‘œ Ã— $0.025/í‰ê°€ = $45/í‰ê°€
- Auto-Improve (12ê°œ ì‹¤í—˜) = $45 Ã— 12 = $540
- ì›”ê°„ 100íšŒ í‰ê°€ = $4,500/ì›”
```

### í•µì‹¬ ê¸°ëŠ¥

1. **Cost Tracking (ë¹„ìš© ì¶”ì )**
   - ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§
   - í‰ê°€ë³„/ì§€í‘œë³„/LLMë³„ ë¹„ìš© ë¶„í•´
   - ì‚¬ìš©ìë³„/í”„ë¡œì íŠ¸ë³„ ë¹„ìš© ì§‘ê³„

2. **Budget Management (ì˜ˆì‚° ê´€ë¦¬)**
   - í”„ë¡œì íŠ¸ë³„ ì˜ˆì‚° ì„¤ì •
   - ì˜ˆì‚° ì´ˆê³¼ ê²½ê³  (50%, 80%, 95% ì„ê³„ê°’)
   - Hard Limit (ì´ˆê³¼ ì‹œ ìë™ ì¤‘ë‹¨)

3. **Cost Prediction (ë¹„ìš© ì˜ˆì¸¡)**
   - í‰ê°€ ìƒì„± ì „ ë¹„ìš© ì¶”ì •
   - ìƒ˜í”Œë§ ì „ëµë³„ ë¹„ìš© ë¹„êµ
   - ì§€í‘œ ì„ íƒì— ë”°ë¥¸ ë¹„ìš© ë³€í™”

4. **Cost Optimization (ë¹„ìš© ìµœì í™”)**
   - ìƒ˜í”Œë§ ì „ëµ (30% ìƒ˜í”Œ â†’ 70% ì ˆê°)
   - ì§€í‘œ ì„ íƒì  í™œì„±í™”
   - LLM ëª¨ë¸ ì „í™˜ (GPT-4o â†’ GPT-4o-mini)
   - ê²°ê³¼ ìºì‹±

---

## ë¹„ìš© ê³„ì‚° ë¡œì§

### LLM ë³„ í† í° ê°€ê²© (2025ë…„ ê¸°ì¤€)

```typescript
const LLM_PRICING = {
  'GPT-4o': {
    provider: 'openai',
    input_price_per_1k: 0.0025,   // $0.0025 per 1K tokens
    output_price_per_1k: 0.01,    // $0.01 per 1K tokens
    cache_price_per_1k: 0.00125   // 50% í• ì¸
  },
  'GPT-4o-mini': {
    provider: 'openai',
    input_price_per_1k: 0.00015,  // $0.00015 per 1K tokens
    output_price_per_1k: 0.0006,  // $0.0006 per 1K tokens
    cache_price_per_1k: 0.000075
  },
  'Claude-3.5 Sonnet': {
    provider: 'anthropic',
    input_price_per_1k: 0.003,    // $0.003 per 1K tokens
    output_price_per_1k: 0.015,   // $0.015 per 1K tokens
    cache_price_per_1k: 0.0015
  },
  'Claude-3 Opus': {
    provider: 'anthropic',
    input_price_per_1k: 0.015,    // $0.015 per 1K tokens
    output_price_per_1k: 0.075,   // $0.075 per 1K tokens
    cache_price_per_1k: 0.0075
  }
};
```

### ì§€í‘œë³„ í‰ê·  í† í° ì†Œë¹„ëŸ‰

LLM Judge ê¸°ë°˜ ì§€í‘œë“¤ì˜ í‰ê·  í† í° ì‚¬ìš©ëŸ‰ (ì‹¤í—˜ ê¸°ë°˜ ì¶”ì •):

```typescript
const METRIC_TOKEN_USAGE = {
  // Generation ì§€í‘œ (ë†’ì€ í† í° ì†Œë¹„)
  'faithfulness': {
    input_tokens: 800,   // Question + Context + Answer + Prompt
    output_tokens: 150   // LLM Judge ì‘ë‹µ
  },
  'answer_relevancy': {
    input_tokens: 700,
    output_tokens: 120
  },
  'answer_correctness': {
    input_tokens: 850,
    output_tokens: 200   // ìƒì„¸í•œ ë¶„ì„ í•„ìš”
  },
  'coherence': {
    input_tokens: 600,
    output_tokens: 100
  },
  'conciseness': {
    input_tokens: 550,
    output_tokens: 80
  },
  
  // Retrieval ì§€í‘œ (ì¤‘ê°„ í† í° ì†Œë¹„)
  'context_precision': {
    input_tokens: 750,
    output_tokens: 130
  },
  'context_recall': {
    input_tokens: 720,
    output_tokens: 140
  },
  'context_entity_recall': {
    input_tokens: 800,
    output_tokens: 160
  },
  
  // Safety ì§€í‘œ (ë†’ì€ í† í° ì†Œë¹„)
  'harmfulness': {
    input_tokens: 900,   // ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ í•„ìš”
    output_tokens: 250
  },
  'maliciousness': {
    input_tokens: 850,
    output_tokens: 230
  },
  
  // Other ì§€í‘œ
  'answer_similarity': {
    input_tokens: 650,
    output_tokens: 100
  },
  'critique_correctness': {
    input_tokens: 900,
    output_tokens: 300   // ê°€ì¥ ë†’ì€ ì¶œë ¥ í† í°
  }
};
```

### ë¹„ìš© ê³„ì‚° í•¨ìˆ˜

```python
def calculate_metric_cost(
    metric_name: str,
    qa_count: int,
    llm_model: str,
    use_caching: bool = False
) -> float:
    """
    íŠ¹ì • ì§€í‘œì˜ í‰ê°€ ë¹„ìš© ê³„ì‚°
    """
    # í† í° ì‚¬ìš©ëŸ‰ ì¡°íšŒ
    token_usage = METRIC_TOKEN_USAGE.get(metric_name)
    if not token_usage:
        raise ValueError(f"Unknown metric: {metric_name}")
    
    # LLM ê°€ê²© ì¡°íšŒ
    pricing = LLM_PRICING.get(llm_model)
    if not pricing:
        raise ValueError(f"Unknown LLM model: {llm_model}")
    
    # ì…ë ¥ í† í° ë¹„ìš©
    if use_caching:
        # ìºì‹± í™œì„±í™” ì‹œ 50% í• ì¸ (ì²« ë²ˆì§¸ ìš”ì²­ ì œì™¸)
        cache_hits = qa_count - 1
        input_cost = (
            token_usage['input_tokens'] * pricing['input_price_per_1k'] / 1000 +
            cache_hits * token_usage['input_tokens'] * pricing['cache_price_per_1k'] / 1000
        )
    else:
        input_cost = (
            qa_count * token_usage['input_tokens'] * pricing['input_price_per_1k'] / 1000
        )
    
    # ì¶œë ¥ í† í° ë¹„ìš©
    output_cost = (
        qa_count * token_usage['output_tokens'] * pricing['output_price_per_1k'] / 1000
    )
    
    return input_cost + output_cost


def calculate_evaluation_cost(
    qa_count: int,
    metrics: list[str],
    llm_model: str = 'GPT-4o',
    sampling_rate: float = 1.0,
    use_caching: bool = False
) -> dict:
    """
    ì „ì²´ í‰ê°€ ë¹„ìš© ê³„ì‚°
    """
    # ìƒ˜í”Œë§ ì ìš©
    effective_qa_count = int(qa_count * sampling_rate)
    
    # ì§€í‘œë³„ ë¹„ìš© ê³„ì‚°
    metric_costs = []
    total_cost = 0
    
    for metric_name in metrics:
        metric_cost = calculate_metric_cost(
            metric_name,
            effective_qa_count,
            llm_model,
            use_caching
        )
        
        metric_costs.append({
            'metric_name': metric_name,
            'cost': metric_cost,
            'qa_count': effective_qa_count
        })
        
        total_cost += metric_cost
    
    return {
        'total_cost': total_cost,
        'cost_per_qa': total_cost / effective_qa_count if effective_qa_count > 0 else 0,
        'qa_count': effective_qa_count,
        'original_qa_count': qa_count,
        'sampling_rate': sampling_rate,
        'metric_costs': metric_costs,
        'use_caching': use_caching
    }


# ì‚¬ìš© ì˜ˆì‹œ
result = calculate_evaluation_cost(
    qa_count=150,
    metrics=[
        'faithfulness',
        'answer_relevancy',
        'context_precision',
        'context_recall'
    ],
    llm_model='GPT-4o',
    sampling_rate=1.0,
    use_caching=False
)

print(f"ì´ ë¹„ìš©: ${result['total_cost']:.2f}")
print(f"QAë‹¹ ë¹„ìš©: ${result['cost_per_qa']:.3f}")

# ì¶œë ¥:
# ì´ ë¹„ìš©: $45.67
# QAë‹¹ ë¹„ìš©: $0.304
```

### ë¹„ìš© ì ˆê° ì‹œë®¬ë ˆì´ì…˜

```python
# ì‹œë‚˜ë¦¬ì˜¤ 1: ì „ì²´ í‰ê°€ (Baseline)
baseline = calculate_evaluation_cost(
    qa_count=150,
    metrics=[
        'faithfulness', 'answer_relevancy', 'answer_correctness',
        'context_precision', 'context_recall', 'context_entity_recall',
        'coherence', 'conciseness', 'harmfulness', 'maliciousness',
        'answer_similarity', 'critique_correctness'
    ],
    llm_model='GPT-4o',
    sampling_rate=1.0,
    use_caching=False
)
print(f"Baseline: ${baseline['total_cost']:.2f}")  # $68.25

# ì‹œë‚˜ë¦¬ì˜¤ 2: 30% ìƒ˜í”Œë§
sampling = calculate_evaluation_cost(
    qa_count=150,
    metrics=[...],  # ë™ì¼
    llm_model='GPT-4o',
    sampling_rate=0.3,
    use_caching=False
)
print(f"30% ìƒ˜í”Œë§: ${sampling['total_cost']:.2f}")  # $20.48 (70% ì ˆê°)

# ì‹œë‚˜ë¦¬ì˜¤ 3: í•„ìˆ˜ ì§€í‘œë§Œ (6ê°œ)
essential_metrics = calculate_evaluation_cost(
    qa_count=150,
    metrics=[
        'faithfulness', 'answer_relevancy',
        'context_precision', 'context_recall',
        'coherence', 'answer_correctness'
    ],
    llm_model='GPT-4o',
    sampling_rate=1.0,
    use_caching=False
)
print(f"í•„ìˆ˜ ì§€í‘œë§Œ: ${essential_metrics['total_cost']:.2f}")  # $34.12 (50% ì ˆê°)

# ì‹œë‚˜ë¦¬ì˜¤ 4: GPT-4o-mini ì „í™˜
mini_model = calculate_evaluation_cost(
    qa_count=150,
    metrics=[...],  # 12ê°œ ì „ì²´
    llm_model='GPT-4o-mini',
    sampling_rate=1.0,
    use_caching=False
)
print(f"GPT-4o-mini: ${mini_model['total_cost']:.2f}")  # $4.09 (94% ì ˆê°!)

# ì‹œë‚˜ë¦¬ì˜¤ 5: ìºì‹± í™œì„±í™”
caching = calculate_evaluation_cost(
    qa_count=150,
    metrics=[...],  # 12ê°œ ì „ì²´
    llm_model='GPT-4o',
    sampling_rate=1.0,
    use_caching=True
)
print(f"ìºì‹± í™œì„±í™”: ${caching['total_cost']:.2f}")  # $54.60 (20% ì ˆê°)

# ì‹œë‚˜ë¦¬ì˜¤ 6: ëª¨ë“  ìµœì í™” ì ìš©
optimized = calculate_evaluation_cost(
    qa_count=150,
    metrics=[
        'faithfulness', 'answer_relevancy',
        'context_precision', 'context_recall',
        'coherence', 'answer_correctness'
    ],
    llm_model='GPT-4o-mini',
    sampling_rate=0.3,
    use_caching=True
)
print(f"ìµœì í™”: ${optimized['total_cost']:.2f}")  # $0.61 (99% ì ˆê°!)
```

---

## API ëª…ì„¸

### 1. ë¹„ìš© ìš”ì•½ ì¡°íšŒ

**Endpoint:** `GET /api/v1/costs/summary`

**Query Parameters:**
- `period` (optional): `today` | `week` | `month` | `all`

**Response:**
```json
{
  "success": true,
  "data": {
    "period": "month",
    "total_cost": 1847.32,
    "total_evaluations": 45,
    "total_qa_processed": 6750,
    "avg_cost_per_evaluation": 41.05,
    "avg_cost_per_qa": 0.27,
    "cost_by_provider": [
      {
        "provider": "openai",
        "model": "GPT-4o",
        "cost": 1142.45,
        "percentage": 61.8
      }
    ],
    "cost_by_metric": [
      {
        "metric_name": "faithfulness",
        "cost": 423.12,
        "percentage": 22.9
      }
    ],
    "cost_trend": [
      {
        "date": "2025-12-07",
        "cost": 234.56
      }
    ]
  }
}
```

### 2. í‰ê°€ë³„ ë¹„ìš© ì¡°íšŒ

**Endpoint:** `GET /api/v1/costs/evaluations/{evaluation_id}`

**Response:**
```json
{
  "success": true,
  "data": {
    "evaluation_id": "eval-001",
    "evaluation_name": "ê³ ê° ì§€ì› QA í‰ê°€",
    "total_cost": 45.67,
    "qa_count": 150,
    "cost_per_qa": 0.304,
    "metric_costs": [
      {
        "metric_name": "faithfulness",
        "llm_model": "GPT-4o",
        "total_calls": 150,
        "input_tokens": 120000,
        "output_tokens": 22500,
        "cached_tokens": 0,
        "cost": 3.56,
        "avg_latency_ms": 1245
      }
    ],
    "timestamp": "2025-12-13T10:30:00Z",
    "duration_minutes": 18,
    "status": "completed"
  }
}
```

### 3. ë¹„ìš© ì˜ˆì¸¡

**Endpoint:** `POST /api/v1/costs/predict`

**Request:**
```json
{
  "dataset_id": "dataset-001",
  "metrics": [
    "faithfulness",
    "answer_relevancy",
    "context_precision",
    "context_recall"
  ],
  "sampling_rate": 0.3,
  "llm_model": "GPT-4o",
  "use_caching": false
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "estimated_cost": 13.70,
    "breakdown": [
      {
        "metric_name": "faithfulness",
        "qa_count": 45,
        "estimated_tokens": 42750,
        "estimated_cost": 3.56
      }
    ],
    "confidence": "high",
    "factors": [
      "ìƒ˜í”Œë§ 30% ì ìš©",
      "150ê°œ QA ì¤‘ 45ê°œ í‰ê°€",
      "ìºì‹± ë¹„í™œì„±í™”"
    ]
  }
}
```

### 4. ì˜ˆì‚° ìƒì„±

**Endpoint:** `POST /api/v1/budgets`

**Request:**
```json
{
  "name": "ì›”ê°„ í‰ê°€ ì˜ˆì‚°",
  "type": "organization",
  "entity_id": "org-001",
  "limit": 2000,
  "period": "monthly",
  "alert_thresholds": [50, 80, 95],
  "is_hard_limit": false
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "id": "budget-001",
    "name": "ì›”ê°„ í‰ê°€ ì˜ˆì‚°",
    "type": "organization",
    "entity_id": "org-001",
    "limit": 2000,
    "period": "monthly",
    "current_usage": 0,
    "percentage_used": 0,
    "alert_thresholds": [50, 80, 95],
    "is_hard_limit": false,
    "created_at": "2025-12-13T10:00:00Z"
  }
}
```

### 5. ì˜ˆì‚° ì•Œë¦¼ ì¡°íšŒ

**Endpoint:** `GET /api/v1/budgets/alerts`

**Query Parameters:**
- `severity` (optional): `info` | `warning` | `critical`
- `is_acknowledged` (optional): `true` | `false`

**Response:**
```json
{
  "success": true,
  "data": {
    "alerts": [
      {
        "id": "alert-001",
        "budget_id": "budget-001",
        "type": "threshold_exceeded",
        "severity": "warning",
        "message": "ì›”ê°„ í‰ê°€ ì˜ˆì‚°ì˜ 92.4%ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤",
        "current_usage": 1847.32,
        "budget_limit": 2000,
        "percentage_used": 92.4,
        "timestamp": "2025-12-13T10:30:00Z",
        "is_acknowledged": false
      }
    ]
  }
}
```

### 6. ë¹„ìš© ìµœì í™” ì œì•ˆ

**Endpoint:** `GET /api/v1/costs/optimize`

**Query Parameters:**
- `evaluation_id` (optional): íŠ¹ì • í‰ê°€ ê¸°ì¤€ ë¶„ì„

**Response:**
```json
{
  "success": true,
  "data": {
    "suggestions": [
      {
        "id": "opt-001",
        "type": "sampling",
        "title": "ìƒ˜í”Œë§ ì „ëµ í™œì„±í™”",
        "description": "ì „ì²´ ë°ì´í„°ì…‹ ëŒ€ì‹  30% ìƒ˜í”Œë§Œ í‰ê°€í•˜ì—¬ ë¹„ìš©ì„ 70% ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "estimated_savings": 1293.12,
        "estimated_savings_percentage": 70,
        "impact_on_accuracy": "ì •í™•ë„ 5% ê°ì†Œ ì˜ˆìƒ (ì‹ ë¢°êµ¬ê°„ Â±2%)",
        "implementation_effort": "easy"
      }
    ]
  }
}
```

---

## ì˜ˆì‚° ê´€ë¦¬ ì „ëµ

### 1. ì¡°ì§ ë ˆë²¨ ì˜ˆì‚°

**ëª©ì :** ì „ì²´ ì¡°ì§ì˜ ì›”ê°„ LLM ë¹„ìš© ìƒí•œ ì„¤ì •

```python
# ì˜ˆì‚° ìƒì„±
budget = {
    "name": "ì¡°ì§ ì›”ê°„ ì˜ˆì‚°",
    "type": "organization",
    "entity_id": "org-001",
    "limit": 5000,  # $5,000/month
    "period": "monthly",
    "alert_thresholds": [50, 80, 95],
    "is_hard_limit": False  # Soft Limit (ê²½ê³ ë§Œ)
}

# ì•Œë¦¼ ì„ê³„ê°’ ì„¤ëª…:
# - 50%: "ì˜ˆì‚°ì˜ ì ˆë°˜ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤" (Info)
# - 80%: "ì˜ˆì‚° ì£¼ì˜ - 80% ì‚¬ìš©" (Warning)
# - 95%: "ì˜ˆì‚° ê±°ì˜ ì†Œì§„ - ì¦‰ì‹œ í™•ì¸ í•„ìš”" (Critical)
```

### 2. í”„ë¡œì íŠ¸ ë ˆë²¨ ì˜ˆì‚°

**ëª©ì :** íŠ¹ì • í”„ë¡œì íŠ¸ì˜ ë¹„ìš© ì œí•œ

```python
# Auto-Improve í”„ë¡œì íŠ¸ ì˜ˆì‚° (Hard Limit)
budget = {
    "name": "Auto-Improve í”„ë¡œì íŠ¸",
    "type": "project",
    "entity_id": "project-auto-improve",
    "limit": 500,  # $500/month
    "period": "monthly",
    "alert_thresholds": [50, 80, 95],
    "is_hard_limit": True  # Hard Limit (ì´ˆê³¼ ì‹œ ì¤‘ë‹¨)
}

# Hard Limit ì‘ë™ ë°©ì‹:
# - ì˜ˆì‚° 95% ë„ë‹¬ ì‹œ: ìƒˆ í‰ê°€ ìƒì„± ì°¨ë‹¨
# - ì‹¤í–‰ ì¤‘ì¸ í‰ê°€: ì™„ë£Œê¹Œì§€ í—ˆìš©
# - ì‚¬ìš©ìì—ê²Œ ì¦‰ì‹œ ì•Œë¦¼ ë°œì†¡
```

### 3. ì‚¬ìš©ì ë ˆë²¨ ì˜ˆì‚°

**ëª©ì :** ê°œë³„ ì‚¬ìš©ìì˜ ê³¼ë„í•œ ë¹„ìš© ë°©ì§€

```python
budget = {
    "name": "ì‚¬ìš©ì ì£¼ê°„ ì˜ˆì‚°",
    "type": "user",
    "entity_id": "user-123",
    "limit": 200,  # $200/week
    "period": "weekly",
    "alert_thresholds": [80, 95],
    "is_hard_limit": False
}
```

### 4. ì˜ˆì‚° ì´ˆê³¼ ì²˜ë¦¬ ë¡œì§

```python
async def check_budget_before_evaluation(
    user_id: str,
    project_id: str,
    estimated_cost: float
) -> dict:
    """
    í‰ê°€ ìƒì„± ì „ ì˜ˆì‚° í™•ì¸
    """
    # 1. ëª¨ë“  ê´€ë ¨ ì˜ˆì‚° ì¡°íšŒ
    budgets = await get_budgets(user_id, project_id)
    
    for budget in budgets:
        # í˜„ì¬ ì‚¬ìš©ëŸ‰ + ì˜ˆìƒ ë¹„ìš©
        projected_usage = budget.current_usage + estimated_cost
        projected_percentage = (projected_usage / budget.limit) * 100
        
        # Hard Limit ì²´í¬
        if budget.is_hard_limit and projected_percentage > 95:
            return {
                "allowed": False,
                "reason": f"{budget.name} ì˜ˆì‚° ì´ˆê³¼ (95% ì´ìƒ)",
                "current_usage": budget.current_usage,
                "limit": budget.limit,
                "estimated_cost": estimated_cost
            }
        
        # Soft Limit ê²½ê³ 
        if projected_percentage > 95:
            # ê²½ê³  ë°œì†¡ (í‰ê°€ëŠ” í—ˆìš©)
            await send_budget_alert(budget, projected_percentage)
    
    return {
        "allowed": True,
        "warnings": [...]
    }
```

---

## ë¹„ìš© ìµœì í™” ë°©ë²•

### 1. ìƒ˜í”Œë§ ì „ëµ (Sampling)

**íš¨ê³¼:** 70% ë¹„ìš© ì ˆê°, 5% ì •í™•ë„ ê°ì†Œ

```python
# Random Sampling
sampling_config = {
    "enabled": True,
    "type": "random",
    "sample_rate": 0.3,  # 30% ìƒ˜í”Œë§
    "min_samples": 50,   # ìµœì†Œ 50ê°œ ë³´ì¥
    "seed": 42           # ì¬í˜„ì„±
}

# Stratified Sampling (ì¹´í…Œê³ ë¦¬ë³„ ê· ë“± ìƒ˜í”Œë§)
sampling_config = {
    "enabled": True,
    "type": "stratified",
    "sample_rate": 0.3,
    "stratify_by": "category",  # ì¹´í…Œê³ ë¦¬ë³„ 30%ì”©
    "min_samples_per_stratum": 10
}

# Systematic Sampling (ì²´ê³„ì  ìƒ˜í”Œë§)
sampling_config = {
    "enabled": True,
    "type": "systematic",
    "sample_rate": 0.3,
    "interval": 3  # ë§¤ 3ë²ˆì§¸ QA ì„ íƒ
}
```

**êµ¬í˜„:**
```python
def apply_sampling(qa_pairs: list, config: dict) -> list:
    """
    ìƒ˜í”Œë§ ì „ëµ ì ìš©
    """
    if not config.get('enabled', False):
        return qa_pairs
    
    sample_size = max(
        int(len(qa_pairs) * config['sample_rate']),
        config.get('min_samples', 1)
    )
    
    if config['type'] == 'random':
        import random
        random.seed(config.get('seed'))
        return random.sample(qa_pairs, sample_size)
    
    elif config['type'] == 'stratified':
        # ì¹´í…Œê³ ë¦¬ë³„ ê· ë“± ìƒ˜í”Œë§
        from collections import defaultdict
        stratified = defaultdict(list)
        
        for qa in qa_pairs:
            category = qa.get(config['stratify_by'], 'default')
            stratified[category].append(qa)
        
        sampled = []
        for category, items in stratified.items():
            n = max(
                int(len(items) * config['sample_rate']),
                config.get('min_samples_per_stratum', 1)
            )
            sampled.extend(random.sample(items, min(n, len(items))))
        
        return sampled
    
    elif config['type'] == 'systematic':
        interval = config['interval']
        return qa_pairs[::interval]
    
    return qa_pairs
```

### 2. ì§€í‘œ ì„ íƒì  í™œì„±í™”

**íš¨ê³¼:** 50% ë¹„ìš© ì ˆê°

```python
# í•„ìˆ˜ ì§€í‘œë§Œ í™œì„±í™”
ESSENTIAL_METRICS = [
    'faithfulness',        # ê°€ì¥ ì¤‘ìš”
    'answer_relevancy',    # ê°€ì¥ ì¤‘ìš”
    'context_precision',   # Retrieval í‰ê°€
    'context_recall',      # Retrieval í‰ê°€
    'answer_correctness',  # Generation í‰ê°€
    'coherence'            # í’ˆì§ˆ í‰ê°€
]

# ì„ íƒì  ì§€í‘œ (í•„ìš” ì‹œë§Œ)
OPTIONAL_METRICS = [
    'harmfulness',         # Safetyê°€ ì¤‘ìš”í•œ ê²½ìš°ë§Œ
    'maliciousness',       # Safetyê°€ ì¤‘ìš”í•œ ê²½ìš°ë§Œ
    'context_entity_recall', # ì„¸ë°€í•œ ë¶„ì„ í•„ìš” ì‹œ
    'answer_similarity',   # ë¹„êµ í‰ê°€ ì‹œ
    'critique_correctness', # ë©”íƒ€ í‰ê°€ ì‹œ
    'conciseness'          # ê°„ê²°ì„± ì¤‘ìš” ì‹œ
]
```

### 3. LLM ëª¨ë¸ ì „í™˜

**íš¨ê³¼:** 60-94% ë¹„ìš© ì ˆê°, 3-8% ì •í™•ë„ ê°ì†Œ

```python
# ì§€í‘œë³„ ê¶Œì¥ LLM ëª¨ë¸
METRIC_LLM_MAPPING = {
    # ë†’ì€ ì •í™•ë„ í•„ìš” â†’ GPT-4o
    'faithfulness': 'GPT-4o',
    'answer_correctness': 'GPT-4o',
    'harmfulness': 'GPT-4o',
    
    # ì¤‘ê°„ ì •í™•ë„ â†’ Claude-3.5 Sonnet (ê· í˜•)
    'answer_relevancy': 'Claude-3.5 Sonnet',
    'context_precision': 'Claude-3.5 Sonnet',
    'context_recall': 'Claude-3.5 Sonnet',
    
    # ë‚®ì€ ì •í™•ë„ í—ˆìš© â†’ GPT-4o-mini (ì €ë¹„ìš©)
    'coherence': 'GPT-4o-mini',
    'conciseness': 'GPT-4o-mini',
    'answer_similarity': 'GPT-4o-mini'
}

# ë¹„ìš© ë¹„êµ
costs = {
    'GPT-4o': calculate_evaluation_cost(150, ESSENTIAL_METRICS, 'GPT-4o'),
    'GPT-4o-mini': calculate_evaluation_cost(150, ESSENTIAL_METRICS, 'GPT-4o-mini'),
    'Mixed': calculate_mixed_model_cost(150, ESSENTIAL_METRICS)
}

print(f"GPT-4o: ${costs['GPT-4o']:.2f}")        # $34.12
print(f"GPT-4o-mini: ${costs['GPT-4o-mini']:.2f}")  # $2.05
print(f"Mixed Models: ${costs['Mixed']:.2f}")   # $18.67 (ìµœì  ê· í˜•)
```

### 4. ê²°ê³¼ ìºì‹±

**íš¨ê³¼:** 20-50% ë¹„ìš© ì ˆê° (ë°˜ë³µ í‰ê°€ ì‹œ)

```python
# ìºì‹± ì „ëµ
CACHING_CONFIG = {
    "enabled": True,
    "cache_key_fields": [
        "question",
        "context",
        "answer",
        "metric_name",
        "llm_model"
    ],
    "ttl_seconds": 86400 * 7,  # 7ì¼
    "max_cache_size_mb": 1000   # 1GB
}

# êµ¬í˜„
import hashlib
import redis

redis_client = redis.Redis()

def get_cached_result(qa_pair: dict, metric: str, model: str) -> dict | None:
    """
    ìºì‹œëœ í‰ê°€ ê²°ê³¼ ì¡°íšŒ
    """
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = hashlib.sha256(
        f"{qa_pair['question']}:{qa_pair['context']}:{qa_pair['answer']}:{metric}:{model}".encode()
    ).hexdigest()
    
    # Redis ì¡°íšŒ
    cached = redis_client.get(f"eval_cache:{cache_key}")
    
    if cached:
        return json.loads(cached)
    
    return None

def cache_result(qa_pair: dict, metric: str, model: str, result: dict):
    """
    í‰ê°€ ê²°ê³¼ ìºì‹±
    """
    cache_key = hashlib.sha256(
        f"{qa_pair['question']}:{qa_pair['context']}:{qa_pair['answer']}:{metric}:{model}".encode()
    ).hexdigest()
    
    redis_client.setex(
        f"eval_cache:{cache_key}",
        CACHING_CONFIG['ttl_seconds'],
        json.dumps(result)
    )
```

### 5. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

**íš¨ê³¼:** 10-15% ë¹„ìš© ì ˆê° (API í˜¸ì¶œ ìµœì í™”)

```python
# ë°°ì¹˜ í‰ê°€ (ë™ì‹œ ì²˜ë¦¬)
async def batch_evaluate(qa_pairs: list, metric: str, batch_size: int = 10):
    """
    ì—¬ëŸ¬ QAë¥¼ ë°°ì¹˜ë¡œ í‰ê°€
    """
    results = []
    
    for i in range(0, len(qa_pairs), batch_size):
        batch = qa_pairs[i:i+batch_size]
        
        # ë™ì‹œ ì²˜ë¦¬
        tasks = [evaluate_single_qa(qa, metric) for qa in batch]
        batch_results = await asyncio.gather(*tasks)
        
        results.extend(batch_results)
    
    return results
```

---

## êµ¬í˜„ ê°€ì´ë“œ

### Phase 1: Cost Tracking (Week 1)

**ë°±ì—”ë“œ êµ¬í˜„:**

```python
# models.py
class EvaluationCost(Base):
    __tablename__ = 'evaluation_costs'
    
    id = Column(String, primary_key=True)
    evaluation_id = Column(String, ForeignKey('evaluations.id'))
    total_cost = Column(Float)
    qa_count = Column(Integer)
    cost_per_qa = Column(Float)
    timestamp = Column(DateTime)
    
    # ì§€í‘œë³„ ë¹„ìš©
    metric_costs = relationship('MetricCost', back_populates='evaluation_cost')

class MetricCost(Base):
    __tablename__ = 'metric_costs'
    
    id = Column(String, primary_key=True)
    evaluation_cost_id = Column(String, ForeignKey('evaluation_costs.id'))
    metric_name = Column(String)
    llm_model = Column(String)
    total_calls = Column(Integer)
    input_tokens = Column(Integer)
    output_tokens = Column(Integer)
    cached_tokens = Column(Integer)
    cost = Column(Float)
    avg_latency_ms = Column(Float)

# cost_tracker.py
class CostTracker:
    def __init__(self):
        self.costs = []
    
    async def track_metric_evaluation(
        self,
        evaluation_id: str,
        metric_name: str,
        llm_model: str,
        input_tokens: int,
        output_tokens: int,
        cached_tokens: int = 0
    ):
        """
        ì§€í‘œ í‰ê°€ ë¹„ìš© ì¶”ì 
        """
        pricing = LLM_PRICING[llm_model]
        
        # ë¹„ìš© ê³„ì‚°
        input_cost = input_tokens * pricing['input_price_per_1k'] / 1000
        output_cost = output_tokens * pricing['output_price_per_1k'] / 1000
        cache_cost = cached_tokens * pricing['cache_price_per_1k'] / 1000
        
        total_cost = input_cost + output_cost + cache_cost
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        metric_cost = MetricCost(
            id=f"mc-{uuid.uuid4()}",
            evaluation_cost_id=evaluation_id,
            metric_name=metric_name,
            llm_model=llm_model,
            total_calls=1,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            cached_tokens=cached_tokens,
            cost=total_cost
        )
        
        await db.add(metric_cost)
        await db.commit()
        
        return total_cost

# FastAPI ì—”ë“œí¬ì¸íŠ¸
@router.get("/costs/summary")
async def get_cost_summary(period: str = 'month'):
    """
    ë¹„ìš© ìš”ì•½ ì¡°íšŒ
    """
    # ê¸°ê°„ë³„ ë¹„ìš© ì§‘ê³„
    costs = await get_costs_by_period(period)
    
    return {
        "success": True,
        "data": {
            "total_cost": sum(c.total_cost for c in costs),
            "total_evaluations": len(costs),
            # ...
        }
    }
```

### Phase 2: Budget Management (Week 2)

```python
# budget_manager.py
class BudgetManager:
    async def check_budget(
        self,
        user_id: str,
        project_id: str,
        estimated_cost: float
    ) -> dict:
        """
        ì˜ˆì‚° ì²´í¬
        """
        budgets = await self.get_active_budgets(user_id, project_id)
        
        for budget in budgets:
            projected = budget.current_usage + estimated_cost
            percentage = (projected / budget.limit) * 100
            
            # Hard Limit ì²´í¬
            if budget.is_hard_limit and percentage > 95:
                return {
                    "allowed": False,
                    "budget": budget,
                    "reason": "Budget limit exceeded"
                }
            
            # ì•Œë¦¼ ì„ê³„ê°’ ì²´í¬
            for threshold in budget.alert_thresholds:
                if budget.percentage_used < threshold <= percentage:
                    await self.send_alert(budget, threshold)
        
        return {"allowed": True}
    
    async def send_alert(self, budget: Budget, threshold: int):
        """
        ì˜ˆì‚° ì•Œë¦¼ ë°œì†¡
        """
        alert = CostAlert(
            id=f"alert-{uuid.uuid4()}",
            budget_id=budget.id,
            type="threshold_exceeded",
            severity="warning" if threshold < 95 else "critical",
            message=f"{budget.name}ì˜ {threshold}%ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤",
            current_usage=budget.current_usage,
            budget_limit=budget.limit,
            percentage_used=budget.percentage_used,
            timestamp=datetime.now().isoformat()
        )
        
        await db.add(alert)
        
        # ì´ë©”ì¼/Slack ì•Œë¦¼
        await send_notification(alert)
```

### Phase 3: Cost Prediction (Week 3)

```python
@router.post("/costs/predict")
async def predict_cost(request: CostPredictRequest):
    """
    í‰ê°€ ë¹„ìš© ì˜ˆì¸¡
    """
    # ë°ì´í„°ì…‹ ì¡°íšŒ
    dataset = await get_dataset(request.dataset_id)
    
    # ë¹„ìš© ê³„ì‚°
    estimated = calculate_evaluation_cost(
        qa_count=len(dataset.qa_pairs),
        metrics=request.metrics,
        llm_model=request.llm_model or 'GPT-4o',
        sampling_rate=request.sampling_rate or 1.0,
        use_caching=request.use_caching or False
    )
    
    return {
        "success": True,
        "data": {
            "estimated_cost": estimated['total_cost'],
            "breakdown": estimated['metric_costs'],
            "confidence": "high",
            "factors": [
                f"ìƒ˜í”Œë§ {int(request.sampling_rate * 100)}% ì ìš©" if request.sampling_rate < 1.0 else "ì „ì²´ í‰ê°€",
                f"{len(dataset.qa_pairs)}ê°œ QA ì¤‘ {estimated['qa_count']}ê°œ í‰ê°€",
                "ìºì‹± í™œì„±í™”" if request.use_caching else "ìºì‹± ë¹„í™œì„±í™”"
            ]
        }
    }
```

---

## ë‹¤ìŒ ë‹¨ê³„

1. **Week 1-3:** Cost Tracking + Budget Management + Cost Prediction êµ¬í˜„
2. **Week 4:** Cost Dashboard í˜ì´ì§€ í”„ë¡ íŠ¸ì—”ë“œ í†µí•©
3. **Week 5:** ë¹„ìš© ìµœì í™” ì œì•ˆ ìë™í™”
4. **Week 6:** í…ŒìŠ¤íŠ¸ ë° ë°°í¬

**í•„ìš”í•œ ì¶”ê°€ ì‘ì—…:**
- [ ] LLM API ì‘ë‹µì—ì„œ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘ (OpenAI/Anthropic API ì‘ë‹µ íŒŒì‹±)
- [ ] Redis ìºì‹± ì¸í”„ë¼ êµ¬ì¶•
- [ ] ì´ë©”ì¼/Slack ì•Œë¦¼ í†µí•©
- [ ] ë¹„ìš© ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (WebSocket)


