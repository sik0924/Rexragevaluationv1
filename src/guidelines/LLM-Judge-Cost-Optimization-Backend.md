# LLM Judge ë¹„ìš© ì ˆê° ë°±ì—”ë“œ êµ¬í˜„ ê°€ì´ë“œ

## ê°œìš”
V1.0ì—ì„œ LLM Judge í˜¸ì¶œ íšŸìˆ˜ë¥¼ 90% ì´ìƒ ì¤„ì´ê¸° ìœ„í•œ 2ë‹¨ê³„ í•„í„°ë§ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ì•„í‚¤í…ì²˜

```
í‰ê°€ ì™„ë£Œ
    â†“
ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¶”ì¶œ
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ì°¨ í•„í„°: íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ìë™ ë¶„ë¥˜ (ë¹„ìš© $0) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ Score Threshold Check
â”‚   â†’ answer_correctness < 0.2 ë˜ëŠ” faithfulness < 0.2
â”‚   â†’ ìë™ ë¶„ë¥˜: "Trivial Failure"
â”‚
â””â”€ Context Volume Check
    â†’ context_recall < 0.1 ë˜ëŠ” context_tokens < 50
    â†’ ìë™ ë¶„ë¥˜: "Retrieval Failure"
    â†“
ì• ë§¤í•œ ì¼€ì´ìŠ¤ë§Œ ì¶”ì¶œ (Ambiguous Cases)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ì°¨ í•„í„°: ìƒ˜í”Œë§ (ë¹„ìš© í†µì œ)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ ìë™ ëª¨ë“œ (mode=auto)
â”‚   â†’ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì— ë”°ë¼ ë¹„ìœ¨ ì¡°ì •
â”‚   â†’ <= 50ê°œ: 100%
â”‚   â†’ 50~200ê°œ: 50%
â”‚   â†’ > 200ê°œ: 20%
â”‚
â”œâ”€ ê³ ì • ë¹„ìœ¨ ëª¨ë“œ (mode=fixed_ratio)
â”‚   â†’ ì§€ì •ëœ ë¹„ìœ¨ë§Œí¼ ë¬´ì‘ìœ„ ìƒ˜í”Œë§
â”‚   â†’ ì˜ˆ: 20% ì§€ì • ì‹œ 100ê°œ ì¤‘ 20ê°œ ì„ íƒ
â”‚
â””â”€ ìµœëŒ€ ì¼€ì´ìŠ¤ ìˆ˜ ëª¨ë“œ (mode=max_cases)
    â†’ ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ ë¬´ì‘ìœ„ ìƒ˜í”Œë§
    â†’ ì˜ˆ: 100ê°œ ì§€ì • ì‹œ ìµœëŒ€ 100ê°œë§Œ ì„ íƒ
    â†“
ì„ íƒëœ ì¼€ì´ìŠ¤ë§Œ LLM Judgeë¡œ ì „ë‹¬
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ì°¨: LLM Judge ìƒì„¸ ë¶„ì„             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
GPT-4ê°€ ê·¼ë³¸ ì›ì¸ ë¶„ì„ + ê°œì„  ì¡°ì–¸
    â†“
ê²°ê³¼ ë³‘í•© ë° DB ì €ì¥
```

## 1. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìˆ˜ì •

### 1.1 failed_cases í…Œì´ë¸” ìˆ˜ì •

```sql
-- ì§„ë‹¨ ë°©ë²• ì¶”ê°€
ALTER TABLE failed_cases 
ADD COLUMN diagnosis_method VARCHAR(20) DEFAULT 'Not Analyzed';
-- ê°€ëŠ¥í•œ ê°’: 'LLM Judge', 'Heuristic', 'Not Analyzed'

-- LLM Judge ìƒ˜í”Œë§ ì—¬ë¶€
ALTER TABLE failed_cases 
ADD COLUMN sampled BOOLEAN DEFAULT FALSE;
-- TRUE: LLM Judgeë¡œ ë¶„ì„ëœ ì¼€ì´ìŠ¤
-- FALSE: íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜ ë˜ëŠ” ë¯¸ë¶„ì„

-- íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜ ì´ìœ 
ALTER TABLE failed_cases 
ADD COLUMN heuristic_reason VARCHAR(100);
-- ì˜ˆ: 'Score < 0.2', 'Context Recall < 0.1', 'Context too short'

-- ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ (íœ´ë¦¬ìŠ¤í‹± í•„í„°ìš©)
ALTER TABLE failed_cases 
ADD COLUMN context_tokens INTEGER;

-- ì¸ë±ìŠ¤ ì¶”ê°€ (ì„±ëŠ¥ ìµœì í™”)
CREATE INDEX idx_diagnosis_method ON failed_cases(diagnosis_method);
CREATE INDEX idx_sampled ON failed_cases(sampled);
```

### 1.2 evaluations í…Œì´ë¸” ìˆ˜ì •

```sql
-- LLM Judge ì„¤ì • ì €ì¥
ALTER TABLE evaluations 
ADD COLUMN llm_judge_config JSONB;
-- ì˜ˆì‹œ JSON:
-- {
--   "enabled": true,
--   "mode": "auto",
--   "fixed_ratio": 20,
--   "max_cases": 100
-- }

-- ì§„ë‹¨ ìš”ì•½ ì •ë³´ (ìºì‹±ìš©)
ALTER TABLE evaluations 
ADD COLUMN diagnosis_summary JSONB;
-- ì˜ˆì‹œ JSON:
-- {
--   "total_failed": 324,
--   "heuristic_classified": 215,
--   "llm_judge_analyzed": 22,
--   "not_analyzed": 87,
--   "diagnosis_cost": 2.40,
--   "breakdown": {
--     "trivial_failures": 180,
--     "retrieval_failures": 35,
--     "ambiguous_cases": 109
--   }
-- }
```

## 2. Python ë°±ì—”ë“œ êµ¬í˜„

### 2.1 ì§„ë‹¨ íŒŒì´í”„ë¼ì¸ (diagnosis_pipeline.py)

```python
# evaluation_engine/diagnosis_pipeline.py

from typing import List, Dict, Tuple
import random
from dataclasses import dataclass

@dataclass
class FailedCaseData:
    """ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë°ì´í„° êµ¬ì¡°"""
    id: str
    question: str
    expected_answer: str
    generated_answer: str
    retrieved_contexts: List[str]
    scores: Dict[str, float]  # 12ê°€ì§€ ì§€í‘œ ì ìˆ˜
    context_tokens: int
    
    # ì§„ë‹¨ ê²°ê³¼ (ì±„ì›Œì§ˆ í•„ë“œ)
    diagnosis_method: str = "Not Analyzed"
    heuristic_reason: str | None = None
    sampled: bool = False
    llm_judge_analysis: Dict | None = None


class DiagnosisPipeline:
    """
    LLM Judge ë¹„ìš© ì ˆê°ì„ ìœ„í•œ 2ë‹¨ê³„ í•„í„°ë§ íŒŒì´í”„ë¼ì¸
    """
    
    # ê¸°ë³¸ ì„ê³„ê°’ (frontendì—ì„œ override ê°€ëŠ¥)
    DEFAULT_THRESHOLDS = {
        'trivial_failure_score': 0.2,
        'retrieval_failure_score': 0.1,
        'min_context_tokens': 50
    }
    
    def __init__(self, llm_judge_client, config: Dict):
        """
        Args:
            llm_judge_client: LLM Judge API í´ë¼ì´ì–¸íŠ¸
            config: {
                'enabled': bool,
                'mode': 'auto' | 'fixed_ratio' | 'max_cases',
                'fixed_ratio': int (0-100),
                'max_cases': int,
                'thresholds': Dict (optional)
            }
        """
        self.llm_judge = llm_judge_client
        self.config = config
        self.thresholds = config.get('thresholds', self.DEFAULT_THRESHOLDS)
    
    def process_failed_cases(
        self, 
        failed_cases: List[FailedCaseData]
    ) -> Tuple[List[FailedCaseData], Dict]:
        """
        ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì§„ë‹¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Returns:
            (processed_cases, diagnosis_summary)
        """
        if not self.config.get('enabled', True):
            # ì§„ë‹¨ ë¹„í™œì„±í™”ëœ ê²½ìš°
            return failed_cases, self._create_summary(failed_cases, [], [])
        
        # 1ë‹¨ê³„: íœ´ë¦¬ìŠ¤í‹± í•„í„°ë§
        ambiguous_cases, heuristic_cases = self._apply_heuristic_filter(failed_cases)
        
        # 2ë‹¨ê³„: ìƒ˜í”Œë§
        sampled_cases = self._apply_sampling(ambiguous_cases)
        
        # 3ë‹¨ê³„: LLM Judge í˜¸ì¶œ
        llm_analyzed_cases = self._run_llm_judge(sampled_cases)
        
        # 4ë‹¨ê³„: ê²°ê³¼ ë³‘í•©
        all_cases = heuristic_cases + llm_analyzed_cases
        not_analyzed_cases = [c for c in ambiguous_cases if c not in sampled_cases]
        all_cases.extend(not_analyzed_cases)
        
        # ì§„ë‹¨ ìš”ì•½ ìƒì„±
        summary = self._create_summary(failed_cases, heuristic_cases, llm_analyzed_cases)
        
        return all_cases, summary
    
    def _apply_heuristic_filter(
        self, 
        cases: List[FailedCaseData]
    ) -> Tuple[List[FailedCaseData], List[FailedCaseData]]:
        """
        1ì°¨ í•„í„°: íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ìë™ ë¶„ë¥˜
        
        Returns:
            (ambiguous_cases, heuristic_classified_cases)
        """
        ambiguous = []
        heuristic_classified = []
        
        for case in cases:
            # Score Threshold Check
            if self._is_trivial_failure(case):
                case.diagnosis_method = "Heuristic"
                case.heuristic_reason = f"Score < {self.thresholds['trivial_failure_score']}"
                # ê°„ë‹¨í•œ ìë™ ë¶„ë¥˜ (LLM Judge ì—†ì´)
                case.llm_judge_analysis = {
                    'failure_type': 'Generation',
                    'reason': 'ì ìˆ˜ê°€ ë§¤ìš° ë‚®ì•„ ëª…ë°±í•œ ì‹¤íŒ¨ë¡œ íŒë‹¨ë¨',
                    'root_cause': {
                        'summary_ko': 'ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.',
                        'advice_ko': 'LLM ëª¨ë¸ ë³€ê²½ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ê³ ë ¤í•˜ì„¸ìš”.'
                    },
                    'diagnosis_method': 'Heuristic'
                }
                heuristic_classified.append(case)
                continue
            
            # Context Volume Check
            if self._is_retrieval_failure(case):
                case.diagnosis_method = "Heuristic"
                case.heuristic_reason = f"Context insufficient"
                case.llm_judge_analysis = {
                    'failure_type': 'Retrieval',
                    'reason': 'ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•¨',
                    'root_cause': {
                        'summary_ko': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì¶©ë¶„íˆ ê²€ìƒ‰í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.',
                        'advice_ko': 'chunk_size ì¦ê°€ ë˜ëŠ” similarity_threshold ë‚®ì¶”ê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.'
                    },
                    'diagnosis_method': 'Heuristic'
                }
                heuristic_classified.append(case)
                continue
            
            # íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ì—†ëŠ” ì• ë§¤í•œ ì¼€ì´ìŠ¤
            ambiguous.append(case)
        
        return ambiguous, heuristic_classified
    
    def _is_trivial_failure(self, case: FailedCaseData) -> bool:
        """ëª…ë°±í•œ ì‹¤íŒ¨ ì—¬ë¶€ íŒë‹¨"""
        threshold = self.thresholds['trivial_failure_score']
        
        # answer_correctness ë˜ëŠ” faithfulnessê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ëª…ë°±í•œ ì‹¤íŒ¨
        if case.scores.get('answer_correctness', 1.0) < threshold:
            return True
        if case.scores.get('faithfulness', 1.0) < threshold:
            return True
        
        return False
    
    def _is_retrieval_failure(self, case: FailedCaseData) -> bool:
        """ê²€ìƒ‰ ì‹¤íŒ¨ ì—¬ë¶€ íŒë‹¨"""
        retrieval_threshold = self.thresholds['retrieval_failure_score']
        min_tokens = self.thresholds['min_context_tokens']
        
        # context_recallì´ ë§¤ìš° ë‚®ê±°ë‚˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê²€ìƒ‰ ì‹¤íŒ¨
        if case.scores.get('context_recall', 1.0) < retrieval_threshold:
            return True
        if case.context_tokens < min_tokens:
            return True
        
        return False
    
    def _apply_sampling(self, cases: List[FailedCaseData]) -> List[FailedCaseData]:
        """
        2ì°¨ í•„í„°: ìƒ˜í”Œë§
        
        Returns:
            ìƒ˜í”Œë§ëœ ì¼€ì´ìŠ¤ ëª©ë¡
        """
        if not cases:
            return []
        
        total = len(cases)
        mode = self.config.get('mode', 'auto')
        
        # ìƒ˜í”Œ í¬ê¸° ê²°ì •
        if mode == 'auto':
            sample_size = self._auto_sample_size(total)
        elif mode == 'fixed_ratio':
            ratio = self.config.get('fixed_ratio', 20) / 100
            sample_size = max(1, int(total * ratio))
        elif mode == 'max_cases':
            max_cases = self.config.get('max_cases', 100)
            sample_size = min(total, max_cases)
        else:
            # ê¸°ë³¸ê°’: ìë™
            sample_size = self._auto_sample_size(total)
        
        # ë¬´ì‘ìœ„ ìƒ˜í”Œë§
        sampled = random.sample(cases, min(sample_size, total))
        
        # sampled í”Œë˜ê·¸ ì„¤ì •
        for case in sampled:
            case.sampled = True
        
        return sampled
    
    def _auto_sample_size(self, total: int) -> int:
        """
        ìë™ ëª¨ë“œì—ì„œ ìƒ˜í”Œ í¬ê¸° ê²°ì •
        
        - <= 50ê°œ: 100% ì „ì²´ ë¶„ì„
        - 50~200ê°œ: 50% ìƒ˜í”Œë§
        - > 200ê°œ: 20% ìƒ˜í”Œë§
        """
        if total <= 50:
            return total  # 100%
        elif total <= 200:
            return int(total * 0.5)  # 50%
        else:
            return int(total * 0.2)  # 20%
    
    def _run_llm_judge(self, cases: List[FailedCaseData]) -> List[FailedCaseData]:
        """
        3ì°¨: LLM Judge í˜¸ì¶œ (ìƒ˜í”Œë§ëœ ì¼€ì´ìŠ¤ë§Œ)
        """
        for case in cases:
            try:
                # LLM Judge API í˜¸ì¶œ
                analysis = self.llm_judge.analyze(
                    user_question=case.question,
                    expected_answer=case.expected_answer,
                    generated_answer=case.generated_answer,
                    retrieved_contexts=case.retrieved_contexts,
                    failed_metric='overall'  # ë˜ëŠ” ê°€ì¥ ë‚®ì€ ì§€í‘œ
                )
                
                case.diagnosis_method = "LLM Judge"
                case.llm_judge_analysis = analysis
                
            except Exception as e:
                # LLM Judge ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ëŒ€ì²´
                print(f"LLM Judge failed for case {case.id}: {e}")
                case.diagnosis_method = "Heuristic"
                case.heuristic_reason = "LLM Judge API Error"
        
        return cases
    
    def _create_summary(
        self, 
        all_cases: List[FailedCaseData],
        heuristic_cases: List[FailedCaseData],
        llm_cases: List[FailedCaseData]
    ) -> Dict:
        """ì§„ë‹¨ ìš”ì•½ ìƒì„±"""
        
        total_failed = len(all_cases)
        heuristic_count = len(heuristic_cases)
        llm_count = len(llm_cases)
        not_analyzed = total_failed - heuristic_count - llm_count
        
        # íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜ ì„¸ë¶€ í†µê³„
        trivial_failures = len([c for c in heuristic_cases if 'Score' in c.heuristic_reason])
        retrieval_failures = len([c for c in heuristic_cases if 'Context' in c.heuristic_reason])
        ambiguous_cases = total_failed - heuristic_count
        
        # ì§„ë‹¨ ë¹„ìš© ê³„ì‚° (GPT-4 ê¸°ì¤€: $0.03/ì¼€ì´ìŠ¤ ê°€ì •)
        diagnosis_cost = llm_count * 0.03
        
        return {
            'total_failed': total_failed,
            'heuristic_classified': heuristic_count,
            'llm_judge_analyzed': llm_count,
            'not_analyzed': not_analyzed,
            'diagnosis_cost': round(diagnosis_cost, 2),
            'breakdown': {
                'trivial_failures': trivial_failures,
                'retrieval_failures': retrieval_failures,
                'ambiguous_cases': ambiguous_cases
            }
        }
```

### 2.2 FastAPI ì—”ë“œí¬ì¸íŠ¸

```python
# api/evaluation_routes.py

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import Dict, Optional

router = APIRouter(prefix="/api/evaluations", tags=["evaluations"])


class LLMJudgeConfig(BaseModel):
    """LLM Judge ì„¤ì •"""
    enabled: bool = True
    mode: str = "auto"  # auto | fixed_ratio | max_cases
    fixed_ratio: Optional[int] = None
    max_cases: Optional[int] = None
    thresholds: Optional[Dict[str, float]] = None


class CreateEvaluationRequest(BaseModel):
    """í‰ê°€ ìƒì„± ìš”ì²­"""
    name: str
    dataset_id: str
    model_id: str
    vector_db_id: str
    metrics: List[Dict]
    rag_system_prompt: str
    rag_hyperparameters: Dict
    llm_judge_config: Optional[LLMJudgeConfig] = LLMJudgeConfig()  # ê¸°ë³¸ê°’: ìë™ ëª¨ë“œ


@router.post("")
async def create_evaluation(
    request: CreateEvaluationRequest,
    current_user = Depends(get_current_user)
):
    """
    í‰ê°€ ìƒì„± ë° ì‹¤í–‰
    """
    # í‰ê°€ ìƒì„±
    evaluation_id = create_evaluation_record(
        name=request.name,
        dataset_id=request.dataset_id,
        model_id=request.model_id,
        vector_db_id=request.vector_db_id,
        metrics=request.metrics,
        rag_config={
            'system_prompt': request.rag_system_prompt,
            'hyperparameters': request.rag_hyperparameters
        },
        llm_judge_config=request.llm_judge_config.dict(),  # ğŸ†• ì €ì¥
        user_id=current_user.id
    )
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ í‰ê°€ ì‹¤í–‰
    background_tasks.add_task(
        run_evaluation_with_diagnosis,
        evaluation_id,
        request.llm_judge_config.dict()
    )
    
    return {
        "success": True,
        "data": {
            "evaluation_id": evaluation_id,
            "status": "pending"
        }
    }


@router.get("/{evaluation_id}/results")
async def get_evaluation_results(
    evaluation_id: str,
    current_user = Depends(get_current_user)
):
    """
    í‰ê°€ ê²°ê³¼ ì¡°íšŒ (ì§„ë‹¨ ìš”ì•½ í¬í•¨)
    """
    # DBì—ì„œ í‰ê°€ ê²°ê³¼ ì¡°íšŒ
    evaluation = get_evaluation_by_id(evaluation_id)
    
    if not evaluation:
        raise HTTPException(status_code=404, detail="Evaluation not found")
    
    # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¡°íšŒ
    failed_cases = get_failed_cases(evaluation_id)
    
    return {
        "success": True,
        "data": {
            "id": evaluation.id,
            "status": evaluation.status,
            "scores": evaluation.scores,
            "diagnosisSummary": evaluation.diagnosis_summary,  # ğŸ†• ì§„ë‹¨ ìš”ì•½
            "failedCases": [
                {
                    "id": case.id,
                    "question": case.question,
                    "expectedAnswer": case.expected_answer,
                    "generatedAnswer": case.generated_answer,
                    "score": case.score,
                    "diagnosisMethod": case.diagnosis_method,  # ğŸ†•
                    "sampled": case.sampled,  # ğŸ†•
                    "llmJudgeAnalysis": case.llm_judge_analysis
                }
                for case in failed_cases
            ]
        }
    }
```

### 2.3 ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…

```python
# evaluation_engine/worker.py

async def run_evaluation_with_diagnosis(
    evaluation_id: str,
    llm_judge_config: Dict
):
    """
    í‰ê°€ ì‹¤í–‰ + LLM Judge ì§„ë‹¨
    """
    try:
        # 1. RAG í‰ê°€ ì‹¤í–‰
        evaluation_results = await run_rag_evaluation(evaluation_id)
        
        # 2. ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¶”ì¶œ
        failed_cases = extract_failed_cases(evaluation_results)
        
        if not failed_cases:
            # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì—†ìœ¼ë©´ ì¢…ë£Œ
            update_evaluation_status(evaluation_id, "completed")
            return
        
        # 3. ì§„ë‹¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = DiagnosisPipeline(
            llm_judge_client=get_llm_judge_client(),
            config=llm_judge_config
        )
        
        diagnosed_cases, summary = pipeline.process_failed_cases(failed_cases)
        
        # 4. DBì— ì €ì¥
        save_failed_cases(evaluation_id, diagnosed_cases)
        update_diagnosis_summary(evaluation_id, summary)
        
        # 5. í‰ê°€ ì™„ë£Œ
        update_evaluation_status(evaluation_id, "completed")
        
    except Exception as e:
        print(f"Evaluation {evaluation_id} failed: {e}")
        update_evaluation_status(evaluation_id, "failed")
```

## 3. ë¹„ìš© ì¶”ì 

### 3.1 ì§„ë‹¨ ë¹„ìš© ê³„ì‚°

```python
# cost_tracking/diagnosis_cost.py

def calculate_diagnosis_cost(llm_judge_calls: int, model: str = "gpt-4") -> float:
    """
    LLM Judge í˜¸ì¶œ ë¹„ìš© ê³„ì‚°
    
    Args:
        llm_judge_calls: LLM Judge í˜¸ì¶œ íšŸìˆ˜
        model: ì‚¬ìš©í•œ LLM ëª¨ë¸
    
    Returns:
        ì´ ë¹„ìš© (USD)
    """
    # ëª¨ë¸ë³„ í‰ê·  ë¹„ìš© (1íšŒ í˜¸ì¶œ ê¸°ì¤€)
    COST_PER_CALL = {
        'gpt-4': 0.035,          # ~1000 í† í° ì…ì¶œë ¥
        'gpt-3.5-turbo': 0.008,
        'claude-3': 0.040
    }
    
    cost_per_call = COST_PER_CALL.get(model, 0.035)
    return round(llm_judge_calls * cost_per_call, 2)
```

## 4. í…ŒìŠ¤íŠ¸

### 4.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# tests/test_diagnosis_pipeline.py

import pytest
from evaluation_engine.diagnosis_pipeline import DiagnosisPipeline, FailedCaseData


def test_heuristic_filter_trivial_failure():
    """ëª…ë°±í•œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìë™ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
    case = FailedCaseData(
        id="test-1",
        question="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
        expected_answer="ì •ë‹µ",
        generated_answer="ì˜¤ë‹µ",
        retrieved_contexts=["ì»¨í…ìŠ¤íŠ¸"],
        scores={'answer_correctness': 0.1},  # < 0.2
        context_tokens=100
    )
    
    pipeline = DiagnosisPipeline(
        llm_judge_client=None,
        config={'enabled': True, 'mode': 'auto'}
    )
    
    ambiguous, heuristic = pipeline._apply_heuristic_filter([case])
    
    assert len(heuristic) == 1
    assert len(ambiguous) == 0
    assert heuristic[0].diagnosis_method == "Heuristic"


def test_sampling_auto_mode():
    """ìë™ ìƒ˜í”Œë§ ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    # 250ê°œ ì¼€ì´ìŠ¤ â†’ 20% ìƒ˜í”Œë§ ì˜ˆìƒ
    cases = [
        FailedCaseData(
            id=f"case-{i}",
            question="ì§ˆë¬¸",
            expected_answer="ì •ë‹µ",
            generated_answer="ì˜¤ë‹µ",
            retrieved_contexts=["ì»¨í…ìŠ¤íŠ¸"],
            scores={'answer_correctness': 0.5},  # ì• ë§¤í•œ ì¼€ì´ìŠ¤
            context_tokens=100
        )
        for i in range(250)
    ]
    
    pipeline = DiagnosisPipeline(
        llm_judge_client=None,
        config={'enabled': True, 'mode': 'auto'}
    )
    
    sampled = pipeline._apply_sampling(cases)
    
    # 20% Â± ì˜¤ì°¨
    assert 45 <= len(sampled) <= 55  # 250 * 0.2 = 50


def test_cost_saving():
    """ë¹„ìš© ì ˆê° íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
    # 1000ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì‹œë®¬ë ˆì´ì…˜
    # - 70% íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜ (700ê°œ)
    # - 30% ì• ë§¤í•œ ì¼€ì´ìŠ¤ (300ê°œ) â†’ 20% ìƒ˜í”Œë§ (60ê°œ)
    
    total_cases = 1000
    heuristic_classified = 700
    llm_judge_analyzed = 60
    
    # ì „ì²´ ë¶„ì„ ì‹œ ì˜ˆìƒ ë¹„ìš©
    full_cost = total_cases * 0.035  # $35
    
    # ì‹¤ì œ ë¹„ìš©
    actual_cost = llm_judge_analyzed * 0.035  # $2.1
    
    # ë¹„ìš© ì ˆê°ìœ¨
    saving_rate = (1 - actual_cost / full_cost) * 100
    
    assert saving_rate > 90  # 90% ì´ìƒ ì ˆê°
```

## 5. ëª¨ë‹ˆí„°ë§

### 5.1 ì§„ë‹¨ íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­

```python
# monitoring/diagnosis_metrics.py

def log_diagnosis_metrics(summary: Dict):
    """
    ì§„ë‹¨ íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ë¡œê¹…
    """
    metrics = {
        'total_failed_cases': summary['total_failed'],
        'heuristic_classified': summary['heuristic_classified'],
        'llm_judge_analyzed': summary['llm_judge_analyzed'],
        'not_analyzed': summary['not_analyzed'],
        'diagnosis_cost': summary['diagnosis_cost'],
        'cost_saving_rate': calculate_saving_rate(summary),
        'heuristic_accuracy': calculate_heuristic_accuracy(summary)
    }
    
    # Prometheus, DataDog ë“±ìœ¼ë¡œ ì „ì†¡
    send_to_monitoring(metrics)
```

## 6. ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] DB ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
- [ ] DiagnosisPipeline í´ë˜ìŠ¤ êµ¬í˜„
- [ ] FastAPI ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •
- [ ] LLM Judge í´ë¼ì´ì–¸íŠ¸ ì—°ë™
- [ ] ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì—…ë°ì´íŠ¸
- [ ] ë¹„ìš© ê³„ì‚° ë¡œì§ ì¶”ê°€
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] í”„ë¡œë•ì…˜ ë°°í¬

## 7. ì˜ˆìƒ íš¨ê³¼

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì†Œê·œëª¨ í‰ê°€ (100ê°œ QA)
- ì‹¤íŒ¨ ì¼€ì´ìŠ¤: 30ê°œ
- íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜: 20ê°œ (67%)
- LLM Judge: 10ê°œ (100% ë¶„ì„)
- **ë¹„ìš©: $0.35** (ì „ì²´ ë¶„ì„ ì‹œ $1.05 â†’ 67% ì ˆê°)

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ê·œëª¨ í‰ê°€ (500ê°œ QA)
- ì‹¤íŒ¨ ì¼€ì´ìŠ¤: 150ê°œ
- íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜: 100ê°œ (67%)
- ì• ë§¤í•œ ì¼€ì´ìŠ¤: 50ê°œ â†’ ìƒ˜í”Œë§ 50%
- LLM Judge: 25ê°œ
- **ë¹„ìš©: $0.88** (ì „ì²´ ë¶„ì„ ì‹œ $5.25 â†’ 83% ì ˆê°)

### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ê·œëª¨ í‰ê°€ (2000ê°œ QA)
- ì‹¤íŒ¨ ì¼€ì´ìŠ¤: 600ê°œ
- íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜: 400ê°œ (67%)
- ì• ë§¤í•œ ì¼€ì´ìŠ¤: 200ê°œ â†’ ìƒ˜í”Œë§ 20%
- LLM Judge: 40ê°œ
- **ë¹„ìš©: $1.40** (ì „ì²´ ë¶„ì„ ì‹œ $21.00 â†’ 93% ì ˆê°)

## 8. í–¥í›„ ê°œì„  ì‚¬í•­ (V1.5)

1. **ì ì‘í˜• ìƒ˜í”Œë§**
   - í†µê³„ì  ì‹ ë¢°ë„ ê¸°ë°˜ ìƒ˜í”Œ í¬ê¸° ìë™ ì¡°ì •
   - Sequential Sampling ê¸°ë²• ì ìš©

2. **íœ´ë¦¬ìŠ¤í‹± ê°œì„ **
   - ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì„ê³„ê°’ ìë™ íŠœë‹
   - ë” ì •êµí•œ ê·œì¹™ ì¶”ê°€

3. **ìºì‹±**
   - ë™ì¼í•œ ì¼€ì´ìŠ¤ ì¬ë¶„ì„ ë°©ì§€
   - í•´ì‹œ ê¸°ë°˜ ê²°ê³¼ ì¬í™œìš©

4. **ì˜ˆì‚° ê¸°ë°˜ ìƒ˜í”Œë§**
   - ì´ ë¹„ìš© í•œë„ ë‚´ì—ì„œ ìµœëŒ€í•œ ìƒ˜í”Œë§
   - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìƒ˜í”Œë§ (ì¤‘ìš”í•œ ì¼€ì´ìŠ¤ ë¨¼ì €)

## ì°¸ê³  ìë£Œ

- í”„ë¡¬í”„íŠ¸ ì „ëµ: `/guidelines/LLM-Judge-Prompt-Strategy.md`
- í”„ë¡ íŠ¸ì—”ë“œ ê°€ì´ë“œ: `/guidelines/LLM-Judge-Sampling-UI-Guide.md`
- íƒ€ì… ì •ì˜: `/types/index.ts`
