# REX 네비게이션 및 워크플로우 가이드

## 📋 개요
이 문서는 REX 시스템의 메뉴 구조, 페이지 간 네비게이션, 그리고 사용자 워크플로우를 설명합니다.

---

## 🗺️ 메뉴 구조 (2024년 10월 기준)

### 네비게이션 순서
메뉴는 사용자의 자연스러운 워크플로우 순서로 정렬되어 있습니다:

```
1. 통합 대시보드      (Dashboard)
2. 데이터셋 관리      (Datasets)          ← 평가 전 선행 단계
3. 평가하기          (Evaluation)        ← 2개 모드로 분리
4. 평가 모니터링      (Monitoring)
5. 평가 이력         (History)
6. 결과 비교         (Comparison)
7. 자동 개선         (Auto-Improve)
8. 비용 대시보드      (Costs)
```

### 메뉴 순서 변경 이유
**이전 순서:** 통합 대시보드 → 평가하기 → ... → 데이터셋 관리  
**현재 순서:** 통합 대시보드 → **데이터셋 관리** → 평가하기 → ...

**변경 배경:**
- 평가를 시작하려면 데이터셋이 선행 조건
- 신규 사용자가 순서대로 따라하기 쉽도록 워크플로우 정합성 확보
- "무엇을 먼저 해야 하지?" → "순서대로 하면 되는구나" 인지 부담 감소

---

## 🎯 평가하기 - 2개 모드

### 모드 선택 페이지 (Evaluation Mode Selection)

평가하기 메뉴를 클릭하면 먼저 2개 모드 중 하나를 선택합니다:

#### 1. 연동된 시스템 평가 (External Evaluation)
**목적:** 기존 운영 중인 RAG 시스템을 평가  
**특징:**
- 외부 RAG API를 연결하여 평가
- 프로덕션 환경의 실제 성능 측정
- 정기적인 성능 모니터링에 적합

**사용 케이스:**
- 운영 중인 챗봇의 주간/월간 성능 평가
- A/B 테스트 결과 비교
- 실시간 프로덕션 시스템 검증

**페이지:** `ExternalEvaluationPageBlue.tsx`

---

#### 2. 신규 평가 (New Evaluation)
**목적:** REX 내부 RAG 파이프라인으로 실험  
**특징:**
- REX에서 직접 RAG 파이프라인 실행
- 다양한 하이퍼파라미터 조합 테스트
- 최적 설정 발견에 적합

**사용 케이스:**
- LLM 모델 선택 실험 (GPT-4 vs Claude)
- Vector DB 비교 (Pinecone vs ChromaDB)
- Retrieval 파라미터 최적화 (Top-K, Similarity Threshold)

**페이지:** `NewEvaluationPageBlue.tsx`

---

## 🚀 핵심 워크플로우

### 워크플로우 1: 첫 평가 실행 (신규 사용자)

```
Step 1: 데이터셋 준비
   ├─ 메뉴: 데이터셋 관리
   ├─ 액션: 데이터셋 생성 또는 업로드
   └─ 결과: QA 페어 리스트 확인

Step 2: 평가 모드 선택
   ├─ 메뉴: 평가하기
   └─ 선택: 연동된 시스템 평가 OR 신규 평가

Step 3: 평가 설정
   ├─ 데이터셋 선택
   ├─ LLM 모델 선택
   ├─ Vector DB 선택
   ├─ 평가 지표 선택 (12개 중)
   └─ LLM Judge 샘플링 설정 (선택)

Step 4: 평가 실행
   ├─ 버튼: "평가 시작"
   └─ 자동 이동: 평가 모니터링 페이지

Step 5: 실시간 모니터링
   ├─ 메뉴: 평가 모니터링
   ├─ 확인: 진행률, 현재 작업, 실시간 로그
   └─ 완료 시: 자동으로 결과 페이지 이동

Step 6: 결과 확인
   ├─ 메뉴: 평가 이력
   ├─ 확인: 12개 지표 점수, 진단 요약
   └─ 다운로드: CSV, JSON 결과 파일
```

---

### 워크플로우 2: 반복 평가 (기존 사용자)

```
빠른 실행 (대시보드에서):
   ├─ 버튼 1: "데이터셋 생성" (필요시만)
   └─ 버튼 2: "평가 시작하기" → 평가 모드 선택

OR

직접 메뉴 이동:
   └─ 메뉴: 평가하기 → 모드 선택 → 설정 → 실행
```

---

### 워크플로우 3: 결과 비교

```
Step 1: 평가 이력 확인
   ├─ 메뉴: 평가 이력
   └─ 선택: 비교할 평가 2개 이상 선택

Step 2: 결과 비교
   ├─ 메뉴: 결과 비교
   ├─ 확인: 지표별 점수 비교 차트
   └─ 분석: 어떤 설정이 더 나은지 판단
```

---

### 워크플로우 4: 자동 개선

```
Step 1: 자동 개선 시작
   ├─ 메뉴: 자동 개선
   ├─ 선택: 개선 대상 평가 결과
   ├─ 설정: 최적화 전략 (빠른 개선/균형 탐색/전체 그리드)
   └─ 실행: "자동 개선 시작"

Step 2: 진행 상황 모니터링
   ├─ 페이지: 자동 개선 진행 상황
   ├─ 확인: 실시간 실험 진행률
   └─ 추적: 현재 최고 점수

Step 3: 결과 확인
   ├─ 페이지: 자동 개선 결과
   ├─ 확인: 최적 하이퍼파라미터 조합
   └─ 적용: "이 설정으로 평가하기" 버튼
```

---

## 🎨 대시보드 빠른 실행 버튼

### Hero 섹션 구성 (평가 시작하기 영역)

**제목:** "평가 시작하기"

**버튼 구성:**
```
┌────────────────────────────────────────┐
│  [데이터셋 생성]  [평가 시작하기]      │
│  (반투명 배경)     (화이트 배경)       │
└────────────────────────────────────────┘
```

**버튼 기능:**
1. **데이터셋 생성** (서브 버튼)
   - 클릭 시: 데이터셋 관리 페이지로 이동
   - 대상 사용자: 신규 사용자 또는 새 데이터셋이 필요한 경우

2. **평가 시작하기** (메인 버튼)
   - 클릭 시: 평가 모드 선택 페이지로 이동
   - 대상 사용자: 반복 평가 사용자 (데이터셋 이미 준비됨)

**디자인 의도:**
- 메인 버튼 = "평가 시작하기" (화이트 배경으로 강조)
- 반복 사용자가 가장 자주 사용하는 액션을 시각적으로 강조
- 신규 사용자도 "데이터셋 생성" 버튼으로 자연스럽게 시작 가능

---

## 📊 페이지별 진입 경로

### 1. 통합 대시보드 (DashboardPageBlue)
**진입 경로:**
- 로그인 후 자동 이동
- 좌측 메뉴 "통합 대시보드" 클릭

**주요 기능:**
- 시스템 종합 현황
- Key Metrics (총 평가 횟수, 평균 점수 등)
- 빠른 실행 버튼 (데이터셋 생성, 평가 시작하기)
- 최근 평가 목록
- 활동 로그

---

### 2. 데이터셋 관리 (DatasetsPageBlue)
**진입 경로:**
- 좌측 메뉴 "데이터셋 관리" 클릭
- 대시보드 "데이터셋 생성" 버튼
- 평가 설정 중 "데이터셋 생성하기" 링크

**주요 기능:**
- 데이터셋 목록 조회
- 새 데이터셋 생성
- 파일 업로드 (CSV, JSON)
- 데이터셋 수정/삭제

---

### 3. 평가 모드 선택 (EvaluationModeSelectionPage)
**진입 경로:**
- 좌측 메뉴 "평가하기" 클릭
- 대시보드 "평가 시작하기" 버튼

**주요 기능:**
- 2개 평가 모드 중 선택
  - 연동된 시스템 평가
  - 신규 평가

**다음 단계:**
- 선택한 모드에 따라 해당 평가 설정 페이지로 이동

---

### 4. 연동된 시스템 평가 (ExternalEvaluationPageBlue)
**진입 경로:**
- 평가 모드 선택 → "연동된 시스템 평가" 선택

**주요 기능:**
- 외부 RAG API 엔드포인트 설정
- 데이터셋 선택
- 평가 지표 선택
- LLM Judge 샘플링 설정
- 평가 실행

**다음 단계:**
- 평가 시작 → 평가 모니터링 페이지

---

### 5. 신규 평가 (NewEvaluationPageBlue)
**진입 경로:**
- 평가 모드 선택 → "신규 평가" 선택

**주요 기능:**
- 데이터셋 선택
- LLM 모델 선택
- Vector DB 선택
- 평가 지표 선택
- LLM Judge 샘플링 설정
- 평가 실행

**다음 단계:**
- 평가 시작 → 평가 모니터링 페이지

---

### 6. 평가 모니터링 (MonitoringPageBlue)
**진입 경로:**
- 좌측 메뉴 "평가 모니터링" 클릭
- 평가 시작 후 자동 이동
- 대시보드 "모니터링 페이지로" 버튼

**주요 기능:**
- 진행 중인 평가 목록
- 실시간 진행률 확인
- 현재 작업 상태 표시
- 평가 중단 기능

**다음 단계:**
- 평가 완료 → 결과 페이지

---

### 7. 평가 이력 (EvaluationHistoryPageBlue)
**진입 경로:**
- 좌측 메뉴 "평가 이력" 클릭
- 대시보드 "전체보기" 버튼
- 평가 완료 후 자동 이동

**주요 기능:**
- 모든 평가 목록 (완료/실패/진행중)
- 필터링 (상태별, 날짜별)
- 평가 상세 결과 보기
- 결과 다운로드 (CSV, JSON)

**다음 단계:**
- 평가 선택 → 결과 페이지
- 여러 평가 선택 → 결과 비교 페이지

---

### 8. 결과 비교 (ComparisonPageBlue)
**진입 경로:**
- 좌측 메뉴 "결과 비교" 클릭
- 평가 이력에서 2개 이상 선택 → "비교하기" 버튼

**주요 기능:**
- 여러 평가 결과 동시 비교
- 지표별 점수 비교 차트
- 설정 차이점 하이라이트
- 최적 설정 추천

---

### 9. 자동 개선 (AutoImproveSetupPageBlue)
**진입 경로:**
- 좌측 메뉴 "자동 개선" 클릭
- 결과 페이지 "자동 개선하기" 버튼

**주요 기능:**
- 개선 대상 평가 선택
- 최적화 전략 선택
- 탐색 범위 설정
- 자동 개선 시작

**다음 단계:**
- 자동 개선 시작 → 진행 상황 페이지 → 결과 페이지

---

### 10. 비용 대시보드 (CostDashboardPageBlue)
**진입 경로:**
- 좌측 메뉴 "비용 대시보드" 클릭
- 대시보드 "이번 달 비용" 카드 클릭

**주요 기능:**
- LLM API 비용 추적
- 모델별 비용 비교
- 예산 설정 및 알림
- 비용 최적화 제안

**하위 페이지:**
- 예산 설정 (BudgetSettingsPageBlue)
- 비용 알림 (CostAlertsPageBlue)

---

## 🔄 페이지 간 데이터 흐름

### Evaluation Store (Zustand)
평가 설정 중 사용자가 입력한 데이터를 관리:

```typescript
// 데이터 설정
useEvaluationStore.getState().setDataset('dataset-001');
useEvaluationStore.getState().setModel('gpt-4o');
useEvaluationStore.getState().setVectorDb('pinecone-1');

// API 요청 객체 생성
const request = useEvaluationStore.getState().getCurrentRequest();

// 평가 생성
await api.evaluations.create(request);
```

### Monitor Store (Zustand)
실시간 평가 모니터링 데이터 관리:

```typescript
// 평가 추가
useMonitorStore.getState().addEvaluation({
  id: 'eval-001',
  name: '2025년 3분기 챗봇 평가',
  status: 'running',
  progress: 0,
});

// 상태 업데이트 (Polling 또는 WebSocket)
useMonitorStore.getState().updateEvaluationStatus('eval-001', {
  progress: 45,
  currentTask: 'Calculating faithfulness...',
});
```

---

## 🎯 사용자 페르소나별 시작 경로

### 신규 사용자 (처음 REX 사용)
```
1. 대시보드 확인 (시스템 현황 파악)
2. "데이터셋 생성" 클릭 → 데이터셋 생성
3. "평가 시작하기" 클릭 → 모드 선택 → 설정 → 실행
4. 모니터링 → 결과 확인
```

### 반복 사용자 (정기 평가)
```
1. 대시보드에서 "평가 시작하기" 바로 클릭
2. 평가 모드 선택 → 설정 → 실행
3. (또는) 좌측 메뉴에서 "평가하기" 바로 클릭
```

### 분석가 (결과 비교 및 개선)
```
1. 평가 이력에서 여러 평가 선택
2. 결과 비교 페이지에서 분석
3. 자동 개선으로 최적화
```

### 관리자 (비용 모니터링)
```
1. 비용 대시보드 확인
2. 예산 설정 및 알림 관리
3. 비용 최적화 제안 검토
```

---

## 📱 Breadcrumb (경로 표시)

모든 페이지 상단에 현재 위치를 표시:

```
홈 > 평가하기 > 신규 평가
홈 > 평가 이력 > 평가 결과
홈 > 자동 개선 > 자동 개선 결과
```

**구현:** `AppLayout.tsx`의 `breadcrumbMap` 참고

---

## 🔗 관련 파일

### 레이아웃 및 네비게이션
- `/components/AppLayout.tsx` - 메뉴 구조 및 네비게이션

### 평가 관련 페이지
- `/components/EvaluationModeSelectionPage.tsx` - 모드 선택
- `/components/ExternalEvaluationPageBlue.tsx` - 연동된 시스템 평가
- `/components/NewEvaluationPageBlue.tsx` - 신규 평가

### 상태 관리
- `/stores/evaluation-store.ts` - 평가 설정 관리
- `/stores/monitor-store.ts` - 모니터링 관리

---

## 📝 변경 이력

### 2024년 10월 24일
- **메뉴 순서 변경:** 데이터셋 관리를 평가하기 앞으로 이동
- **대시보드 버튼 변경:** "결과 비교하기" 제거, "데이터셋 생성" 추가
- **버튼 강조 수정:** "평가 시작하기"를 메인 버튼으로 강조 (화이트 배경)

---

## 🚀 다음 단계

이 가이드는 프론트엔드 네비게이션 구조를 설명합니다.  
백엔드 API 연동은 다음 문서를 참고하세요:

- [API 명세서](./API-Specification.md)
- [백엔드 연동 가이드](./Backend-Integration-Complete-Guide.md)
- [아키텍처 개요](./Architecture-Overview.md)
