# REX Auto-Improve ì•Œê³ ë¦¬ì¦˜ ëª…ì„¸ì„œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [Level 1: Rule-based Optimization (ì¶”ì²œ)](#level-1-rule-based-optimization-ì¶”ì²œ)
3. [Level 2: Sequential Greedy Optimization](#level-2-sequential-greedy-optimization)
4. [Level 3: Bayesian Optimization](#level-3-bayesian-optimization)
5. [êµ¬í˜„ ë¡œë“œë§µ](#êµ¬í˜„-ë¡œë“œë§µ)
6. [API ì„¤ê³„](#api-ì„¤ê³„)

---

## ê°œìš”

### ë¬¸ì œ ì •ì˜
RAG ì‹œìŠ¤í…œì˜ 12ê°œ ì§€í‘œ ì¤‘ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì§€í‘œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, Retrieval ë° Generation íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.

### í•µì‹¬ ë„ì „ ê³¼ì œ
- **íƒìƒ‰ ê³µê°„:** 6ê°œ íŒŒë¼ë¯¸í„° Ã— í‰ê·  4ê°œ ì˜µì…˜ = 4,096ê°€ì§€ ì¡°í•©
- **í‰ê°€ ë¹„ìš©:** 1íšŒ í‰ê°€ = 150 QA Ã— $0.01 = $1.5, ì „ì²´ íƒìƒ‰ = $6,144
- **ì‹œê°„ ì œì•½:** 1íšŒ í‰ê°€ = 15ë¶„, ì „ì²´ íƒìƒ‰ = 1,024ì‹œê°„ (42ì¼)

### í•´ê²° ì „ëµ
- **Level 1:** ê·œì¹™ ê¸°ë°˜ + ì œí•œì  Grid Search â†’ 12-24íšŒ í‰ê°€ (3-6ì‹œê°„, $18-36)
- **Level 2:** Greedy ìˆœì°¨ ìµœì í™” â†’ 18-30íšŒ í‰ê°€ (5-8ì‹œê°„, $27-45)
- **Level 3:** Bayesian Optimization â†’ 30-50íšŒ í‰ê°€ (8-13ì‹œê°„, $45-75)

---

## Level 1: Rule-based Optimization (ì¶”ì²œ)

### âœ… ì¶”ì²œ ì´ìœ 
- **ê°œë°œ ê¸°ê°„:** 1-2ì£¼
- **êµ¬í˜„ ë‚œì´ë„:** â­â­â˜†â˜†â˜† (ì‰¬ì›€)
- **ì„±ëŠ¥:** í‰ê·  15-25% ê°œì„ 
- **ë¹„ìš©:** $18-36
- **MVPì— ì í•©:** ì¦‰ì‹œ ì‹¤ìš© ê°€ëŠ¥

### ğŸ“Š ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„

#### Step 1: ê·¼ë³¸ ì›ì¸ ë¶„ì„ (Root Cause Analysis)

**ì…ë ¥:** ê¸°ì¤€ í‰ê°€ ê²°ê³¼ (Baseline Evaluation)

**ì§€í‘œ â†’ ì›ì¸ ë§¤í•‘ í…Œì´ë¸”:**
| ì§€í‘œ | ì„ê³„ê°’ | ê·¼ë³¸ ì›ì¸ | ê´€ë ¨ íŒŒë¼ë¯¸í„° |
|------|--------|-----------|--------------|
| Context Precision | < 0.7 | Retrieval í’ˆì§ˆ | `top_k`, `chunk_size` |
| Context Recall | < 0.7 | Retrieval ë²”ìœ„ | `top_k`, `embedding_model` |
| Context Entity Recall | < 0.7 | Retrieval ì„¸ë°€ë„ | `chunk_size`, `embedding_model` |
| Faithfulness | < 0.7 | Generation í™˜ê° | `temperature`, `llm_model` |
| Answer Relevancy | < 0.7 | Generation ì •í™•ë„ | `temperature`, `llm_model` |
| Answer Correctness | < 0.7 | Generation í’ˆì§ˆ | `llm_model`, `max_tokens` |
| Coherence | < 0.7 | Generation ì¼ê´€ì„± | `temperature` |
| Conciseness | < 0.7 | Generation ê°„ê²°ì„± | `max_tokens`, `temperature` |

**ì¶œë ¥:** ë¬¸ì œ ì¹´í…Œê³ ë¦¬ ë° ìš°ì„ ìˆœìœ„ íŒŒë¼ë¯¸í„°
```typescript
{
  "root_causes": {
    "retrieval": {
      "severity": "high",  // low, medium, high
      "affected_metrics": ["context_recall", "context_precision"],
      "priority_params": ["top_k", "chunk_size"]
    },
    "generation": {
      "severity": "medium",
      "affected_metrics": ["faithfulness"],
      "priority_params": ["temperature"]
    }
  },
  "recommended_strategy": "retrieval_first"  // retrieval_first, generation_first, balanced
}
```

#### Step 2: íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„± (Smart Grid Search)

**ì „ëµ 1: Retrieval ìš°ì„  (retrieval_first)**
```typescript
// Severityê°€ 'high'ì¸ ê²½ìš°
const retrievalExperiments = [
  // 1. Top-K ìµœì í™” (Chunk Size ê³ ì •)
  { top_k: 3, chunk_size: 512 },   // ë² ì´ìŠ¤ë¼ì¸ ì„¤ì •
  { top_k: 5, chunk_size: 512 },
  { top_k: 10, chunk_size: 512 },
  
  // 2. Chunk Size ìµœì í™” (Top-K ê³ ì • - ì´ì „ ìµœê³ ê°’)
  { top_k: 5, chunk_size: 256 },   // 5ê°€ ìµœê³ ì˜€ë‹¤ê³  ê°€ì •
  { top_k: 5, chunk_size: 1024 },
  
  // 3. Embedding ëª¨ë¸ ë³€ê²½ (Top-K, Chunk Size ê³ ì •)
  { top_k: 5, chunk_size: 512, embedding_model: 'text-embedding-3-large' }
];
// ì´ 6íšŒ í‰ê°€ ($9, 1.5ì‹œê°„)
```

**ì „ëµ 2: Generation ìš°ì„  (generation_first)**
```typescript
const generationExperiments = [
  // 1. Temperature ìµœì í™” (LLM ê³ ì •)
  { temperature: 0.1, llm_model: 'GPT-4o' },
  { temperature: 0.3, llm_model: 'GPT-4o' },
  { temperature: 0.5, llm_model: 'GPT-4o' },
  { temperature: 0.7, llm_model: 'GPT-4o' },
  
  // 2. LLM ëª¨ë¸ ë³€ê²½ (Temperature ê³ ì • - ì´ì „ ìµœê³ ê°’)
  { temperature: 0.3, llm_model: 'Claude-3.5 Sonnet' },
  { temperature: 0.3, llm_model: 'GPT-4o-mini' },
  
  // 3. Max Tokens ì¡°ì •
  { temperature: 0.3, llm_model: 'GPT-4o', max_tokens: 256 },
  { temperature: 0.3, llm_model: 'GPT-4o', max_tokens: 512 }
];
// ì´ 8íšŒ í‰ê°€ ($12, 2ì‹œê°„)
```

**ì „ëµ 3: ê· í˜• ì ‘ê·¼ (balanced)**
```typescript
// Severityê°€ ë‘˜ ë‹¤ 'medium'ì¸ ê²½ìš°
const balancedExperiments = [
  // Phase 1: Retrieval (4íšŒ)
  { top_k: 3 },
  { top_k: 5 },
  { top_k: 10 },
  { chunk_size: 256 },  // Top-KëŠ” ì´ì „ ìµœê³ ê°’ ì‚¬ìš©
  
  // Phase 2: Generation (4íšŒ)
  { temperature: 0.3 },
  { temperature: 0.5 },
  { temperature: 0.7 },
  { llm_model: 'Claude-3.5 Sonnet' }
];
// ì´ 8íšŒ í‰ê°€ ($12, 2ì‹œê°„)
```

#### Step 3: í‰ê°€ ì‹¤í–‰ ë° ì¡°ê¸° ì¢…ë£Œ

**Early Stopping ê·œì¹™:**
```typescript
interface EarlyStoppingConfig {
  min_improvement: 0.05;      // ìµœì†Œ 5% ê°œì„ 
  patience: 3;                 // 3íšŒ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
  target_score: 0.9;          // ëª©í‘œ ì ìˆ˜ ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
}

// ì˜ˆì‹œ
if (currentScore >= baselineScore * 1.05) {
  console.log('ê°œì„  ë‹¬ì„±! ë‹¤ìŒ íŒŒë¼ë¯¸í„°ë¡œ ì§„í–‰');
} else if (noImprovementCount >= 3) {
  console.log('ì¡°ê¸° ì¢…ë£Œ: ê°œì„  ì—†ìŒ');
  return bestConfigSoFar;
} else if (currentScore >= 0.9) {
  console.log('ëª©í‘œ ë‹¬ì„±! ìµœì í™” ì™„ë£Œ');
  return currentConfig;
}
```

#### Step 4: ìµœì¢… ê²€ì¦ ë° ê¶Œì¥ ì„¤ì •

**ì¶œë ¥:**
```typescript
{
  "best_config": {
    "top_k": 5,
    "chunk_size": 512,
    "embedding_model": "text-embedding-3-large",
    "temperature": 0.3,
    "llm_model": "Claude-3.5 Sonnet",
    "max_tokens": 512
  },
  "improvement": {
    "baseline_score": 0.72,
    "best_score": 0.89,
    "improvement_rate": 0.236,  // 23.6% ê°œì„ 
    "improved_metrics": {
      "context_recall": { "before": 0.65, "after": 0.88 },
      "faithfulness": { "before": 0.70, "after": 0.92 }
    }
  },
  "experiments_run": 12,
  "total_cost": 18.00,
  "duration_minutes": 180
}
```

---

## Level 2: Sequential Greedy Optimization

### âœ… íŠ¹ì§•
- **ê°œë°œ ê¸°ê°„:** 2-4ì£¼
- **êµ¬í˜„ ë‚œì´ë„:** â­â­â­â˜†â˜† (ì¤‘ê°„)
- **ì„±ëŠ¥:** í‰ê·  25-35% ê°œì„ 
- **ë¹„ìš©:** $27-45

### ğŸ“Š ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„

#### Greedy ìˆœì°¨ ìµœì í™” ì „ëµ

**í•µì‹¬ ì•„ì´ë””ì–´:**
1. í•œ ë²ˆì— í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ ìµœì í™”
2. ì´ì „ ë‹¨ê³„ì˜ ìµœì ê°’ì„ ë‹¤ìŒ ë‹¨ê³„ì— ì ìš©
3. íŒŒë¼ë¯¸í„° ê°„ ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ìˆœì„œ ê²°ì •

**íŒŒë¼ë¯¸í„° ìµœì í™” ìˆœì„œ:**
```
1. Embedding Model (ì˜í–¥ë ¥ í¼, ì˜µì…˜ ì ìŒ) â†’ 3íšŒ í‰ê°€
2. Chunk Size (Embeddingì— ì˜ì¡´) â†’ 4íšŒ í‰ê°€
3. Top-K (Chunk Sizeì— ì˜ì¡´) â†’ 4íšŒ í‰ê°€
4. LLM Model (ë…ë¦½ì ) â†’ 4íšŒ í‰ê°€
5. Temperature (LLMì— ì˜ì¡´) â†’ 5íšŒ í‰ê°€
6. Max Tokens (Temperatureì— ì˜ì¡´) â†’ 4íšŒ í‰ê°€

ì´ 24íšŒ í‰ê°€
```

**ì˜ì‚¬ ì½”ë“œ:**
```typescript
async function sequentialGreedyOptimization(
  baselineEvaluation: Evaluation
): Promise<OptimizationResult> {
  let currentConfig = baselineEvaluation.config;
  let currentScore = baselineEvaluation.avgScore;
  
  const parameterSequence = [
    { name: 'embedding_model', options: ['ada-002', '3-small', '3-large'] },
    { name: 'chunk_size', options: [128, 256, 512, 1024] },
    { name: 'top_k', options: [3, 5, 10, 15] },
    { name: 'llm_model', options: ['GPT-4o', 'GPT-4o-mini', 'Claude-3.5 Sonnet'] },
    { name: 'temperature', options: [0.1, 0.3, 0.5, 0.7, 0.9] },
    { name: 'max_tokens', options: [128, 256, 512, 1024] }
  ];
  
  for (const param of parameterSequence) {
    console.log(`Optimizing ${param.name}...`);
    
    let bestOptionScore = currentScore;
    let bestOption = currentConfig[param.name];
    
    for (const option of param.options) {
      // í˜„ì¬ ìµœì  ì„¤ì •ì—ì„œ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ ë³€ê²½
      const testConfig = { ...currentConfig, [param.name]: option };
      
      // í‰ê°€ ì‹¤í–‰
      const score = await runEvaluation(testConfig);
      
      if (score > bestOptionScore) {
        bestOptionScore = score;
        bestOption = option;
      }
    }
    
    // ìµœì ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    currentConfig[param.name] = bestOption;
    currentScore = bestOptionScore;
    
    console.log(`Best ${param.name}: ${bestOption} (score: ${bestOptionScore})`);
  }
  
  return {
    best_config: currentConfig,
    final_score: currentScore
  };
}
```

**ì¥ì :**
- íŒŒë¼ë¯¸í„° ê°„ ìƒí˜¸ì‘ìš© ê³ ë ¤
- ì „ì²´ Grid Search ëŒ€ë¹„ 1/170ë¡œ í‰ê°€ íšŸìˆ˜ ê°ì†Œ
- êµ¬í˜„ì´ ì§ê´€ì 

**ë‹¨ì :**
- Local Optimaì— ë¹ ì§ˆ ê°€ëŠ¥ì„±
- íŒŒë¼ë¯¸í„° ìˆœì„œì— ë¯¼ê°

---

## Level 3: Bayesian Optimization

### âœ… íŠ¹ì§•
- **ê°œë°œ ê¸°ê°„:** 4-8ì£¼
- **êµ¬í˜„ ë‚œì´ë„:** â­â­â­â­â­ (ì–´ë ¤ì›€)
- **ì„±ëŠ¥:** í‰ê·  30-40% ê°œì„ 
- **ë¹„ìš©:** $45-75

### ğŸ“Š ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„

#### Bayesian Optimization with Gaussian Process

**í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:**
```bash
# Python ë°±ì—”ë“œ
pip install scikit-optimize  # Bayesian Optimization
pip install optuna           # Alternative (ì¶”ì²œ)
```

**Optuna ê¸°ë°˜ êµ¬í˜„:**
```python
import optuna
from optuna.samplers import TPESampler

def objective(trial):
    # íŒŒë¼ë¯¸í„° ì œì•ˆ
    config = {
        'embedding_model': trial.suggest_categorical(
            'embedding_model', 
            ['ada-002', '3-small', '3-large']
        ),
        'chunk_size': trial.suggest_int('chunk_size', 128, 1024, step=128),
        'top_k': trial.suggest_int('top_k', 3, 15),
        'llm_model': trial.suggest_categorical(
            'llm_model',
            ['GPT-4o', 'GPT-4o-mini', 'Claude-3.5 Sonnet']
        ),
        'temperature': trial.suggest_float('temperature', 0.1, 0.9),
        'max_tokens': trial.suggest_int('max_tokens', 128, 1024, step=128)
    }
    
    # í‰ê°€ ì‹¤í–‰
    score = run_evaluation(config)
    
    return score

# Bayesian Optimization ì‹¤í–‰
study = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42)
)

study.optimize(
    objective,
    n_trials=50,           # 50íšŒ í‰ê°€
    timeout=3600 * 12,     # 12ì‹œê°„ ì œí•œ
    callbacks=[
        # Early Stopping
        optuna.pruners.MedianPruner(n_startup_trials=10)
    ]
)

print(f"Best config: {study.best_params}")
print(f"Best score: {study.best_value}")
```

**ì¥ì :**
- ìµœì†Œ í‰ê°€ íšŸìˆ˜ë¡œ ìµœì í•´ íƒìƒ‰
- íŒŒë¼ë¯¸í„° ê°„ ë³µì¡í•œ ìƒí˜¸ì‘ìš© í•™ìŠµ
- Early Stopping ì§€ì›

**ë‹¨ì :**
- êµ¬í˜„ ë³µì¡ë„ ë†’ìŒ
- Python ë°±ì—”ë“œ í•„ìˆ˜
- ì†Œê·œëª¨ ë°ì´í„°ì…‹ì—ëŠ” ì˜¤ë²„í‚¬

---

## êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Level 1 êµ¬í˜„ (Week 1-2)

**Week 1: ë°±ì—”ë“œ API**
- [ ] Root Cause Analysis API
- [ ] Smart Grid Search ë¡œì§
- [ ] Early Stopping êµ¬í˜„

**Week 2: í”„ë¡ íŠ¸ì—”ë“œ í†µí•©**
- [ ] AutoImproveSetupPage ë¡œì§ ì—°ë™
- [ ] AutoImproveProgressPage ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- [ ] AutoImproveResultsPage ê°œì„  ë¹„êµ ì°¨íŠ¸

### Phase 2: Level 2 êµ¬í˜„ (Week 3-6)

**Week 3-4: Sequential Greedy**
- [ ] íŒŒë¼ë¯¸í„° ì˜ì¡´ì„± ê·¸ë˜í”„ ì •ì˜
- [ ] ìˆœì°¨ ìµœì í™” ì—”ì§„

**Week 5-6: ê³ ê¸‰ ê¸°ëŠ¥**
- [ ] íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ë¶„ì„
- [ ] ë¯¼ê°ë„ ë¶„ì„ (Sensitivity Analysis)

### Phase 3: Level 3 êµ¬í˜„ (Week 7-12)

**Week 7-9: Optuna í†µí•©**
- [ ] Python ë°±ì—”ë“œ Optuna ì„¤ì •
- [ ] Hyperparameter ê³µê°„ ì •ì˜
- [ ] ë³‘ë ¬ í‰ê°€ ï¿½ï¿½ì›

**Week 10-12: ìµœì í™” ë° ì‹œê°í™”**
- [ ] ìµœì í™” íˆìŠ¤í† ë¦¬ ì‹œê°í™”
- [ ] íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ í”Œë¡¯
- [ ] Optuna Dashboard í†µí•©

---

## API ì„¤ê³„

### 1. Root Cause Analysis API

**Endpoint:** `POST /api/v1/auto-improve/analyze`

**Request:**
```json
{
  "evaluation_id": "eval-001",
  "target_metrics": ["context_recall", "faithfulness"]
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "root_causes": {
      "retrieval": {
        "severity": "high",
        "affected_metrics": ["context_recall"],
        "scores": {
          "context_recall": 0.62,
          "context_precision": 0.75
        },
        "priority_params": ["top_k", "chunk_size"]
      },
      "generation": {
        "severity": "medium",
        "affected_metrics": ["faithfulness"],
        "scores": {
          "faithfulness": 0.68,
          "answer_relevancy": 0.80
        },
        "priority_params": ["temperature", "llm_model"]
      }
    },
    "recommended_strategy": "retrieval_first",
    "estimated_experiments": 12,
    "estimated_cost": 18.00,
    "estimated_duration_minutes": 180
  }
}
```

### 2. Generate Experiments API

**Endpoint:** `POST /api/v1/auto-improve/generate-experiments`

**Request:**
```json
{
  "base_evaluation_id": "eval-001",
  "strategy": "retrieval_first",
  "optimization_level": "rule_based",
  "budget": {
    "max_experiments": 20,
    "max_cost": 30.00,
    "max_duration_minutes": 300
  }
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "job_id": "auto-improve-job-001",
    "experiments": [
      {
        "id": "exp-001",
        "name": "Baseline",
        "config": {
          "top_k": 5,
          "chunk_size": 512,
          "temperature": 0.7
        },
        "order": 1
      },
      {
        "id": "exp-002",
        "name": "Top-K=3",
        "config": {
          "top_k": 3,
          "chunk_size": 512,
          "temperature": 0.7
        },
        "order": 2
      }
    ],
    "total_experiments": 12,
    "estimated_cost": 18.00
  }
}
```

### 3. Start Auto-Improve Job API

**Endpoint:** `POST /api/v1/auto-improve/jobs`

**Request:**
```json
{
  "base_evaluation_id": "eval-001",
  "strategy": "retrieval_first",
  "optimization_level": "rule_based",
  "early_stopping": {
    "enabled": true,
    "min_improvement": 0.05,
    "patience": 3
  }
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "job_id": "auto-improve-job-001",
    "status": "pending",
    "created_at": "2025-10-13T10:00:00Z",
    "websocket_url": "wss://api.rex.com/ws/auto-improve/auto-improve-job-001"
  }
}
```

### 4. Get Auto-Improve Results API

**Endpoint:** `GET /api/v1/auto-improve/jobs/{job_id}`

**Response:**
```json
{
  "success": true,
  "data": {
    "job_id": "auto-improve-job-001",
    "status": "completed",
    "experiments_completed": 12,
    "best_config": {
      "top_k": 5,
      "chunk_size": 512,
      "embedding_model": "text-embedding-3-large",
      "temperature": 0.3,
      "llm_model": "Claude-3.5 Sonnet"
    },
    "improvement": {
      "baseline_score": 0.72,
      "best_score": 0.89,
      "improvement_rate": 0.236
    },
    "detailed_results": [
      {
        "experiment_id": "exp-001",
        "config": { "top_k": 5 },
        "score": 0.72
      },
      {
        "experiment_id": "exp-002",
        "config": { "top_k": 3 },
        "score": 0.75
      }
    ]
  }
}
```

---

## ìµœì¢… ì¶”ì²œ

### ğŸ¯ MVP (ìµœì†Œ ê¸°ëŠ¥ ì œí’ˆ)
**â†’ Level 1: Rule-based Optimization**

**ì´ìœ :**
1. âœ… 2ì£¼ ë‚´ êµ¬í˜„ ê°€ëŠ¥
2. âœ… ì¦‰ì‹œ ì‹¤ìš©ì ì¸ ê²°ê³¼ (15-25% ê°œì„ )
3. âœ… ë‚®ì€ ë¹„ìš© ($18-36)
4. âœ… ì‚¬ìš©ì ì´í•´í•˜ê¸° ì‰¬ì›€

**êµ¬í˜„ ìš°ì„ ìˆœìœ„:**
1. Week 1: Root Cause Analysis + Smart Grid Search
2. Week 2: í”„ë¡ íŠ¸ì—”ë“œ í†µí•© + Early Stopping
3. Week 3-4: ì„±ëŠ¥ ìµœì í™” + ì‚¬ìš©ì í…ŒìŠ¤íŠ¸

### ğŸš€ Long-term (ì¥ê¸° ëª©í‘œ)
**â†’ Level 2: Sequential Greedy**

**ì´ìœ :**
1. âœ… Level 1 ëŒ€ë¹„ 10-15% ì¶”ê°€ ê°œì„ 
2. âœ… ì¶”ê°€ 4ì£¼ íˆ¬ìë¡œ í° íš¨ê³¼
3. âœ… ì—¬ì „íˆ ì´í•´ ê°€ëŠ¥í•œ ë¡œì§

### ğŸ”¬ Research (ì—°êµ¬ í”„ë¡œì íŠ¸)
**â†’ Level 3: Bayesian Optimization**

**ì¡°ê±´:**
- ëŒ€ê·œëª¨ ì‚¬ìš©ì ë² ì´ìŠ¤ (ì›” 1000íšŒ ì´ìƒ í‰ê°€)
- ì „ë¬¸ ë°ì´í„° ê³¼í•™íŒ€ ë³´ìœ 
- ì¶©ë¶„í•œ ê°œë°œ ë¦¬ì†ŒìŠ¤ (8ì£¼ ì´ìƒ)

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ì˜ì‚¬ ê²°ì •:** Level 1, 2, 3 ì¤‘ ì„ íƒ
2. **API êµ¬í˜„:** `/guidelines/API-Specification.md`ì— ì¶”ê°€
3. **í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™:** ê¸°ì¡´ í˜ì´ì§€ì™€ í†µí•©
4. **í…ŒìŠ¤íŠ¸:** Mock ë°ì´í„°ë¡œ ì›Œí¬í”Œë¡œìš° ê²€ì¦

**ì§ˆë¬¸ì´ë‚˜ ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•˜ë©´ ë§ì”€í•´ì£¼ì„¸ìš”!**
